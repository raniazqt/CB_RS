{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/env python\n",
    "# -*- coding: latin-1 -*-\n",
    "import pandas as pd # pandas is a data manipulation library\n",
    "import numpy as np #provides numerical arrays and functions to manipulate the arrays efficiently\n",
    "import logging\n",
    "import difflib\n",
    "import Levenshtein as lev\n",
    "from tqdm.notebook import tqdm\n",
    "import multiprocessing\n",
    "import gensim.models.doc2vec\n",
    "import gensim\n",
    "from gensim.models.doc2vec import Doc2Vec, TaggedDocument\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "import scipy\n",
    "from scipy import spatial\n",
    "import re\n",
    "import movies_df_helper_functions as helper_fn\n",
    "import evaluation_helper_functions as eval_fn\n",
    "import random\n",
    "import imp\n",
    "import itertools\n",
    "from sklearn.manifold import TSNE\n",
    "from sklearn.cluster import AffinityPropagation\n",
    "from nltk.translate.bleu_score import sentence_bleu\n",
    "from nltk.translate.bleu_score import SmoothingFunction\n",
    "\n",
    "from tabulate import tabulate\n",
    "import decimal\n",
    "\n",
    "import csv\n",
    "import imp\n",
    "imp.reload(eval_fn)\n",
    "imp.reload(helper_fn)\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\programs\\python39\\lib\\site-packages\\tqdm\\std.py:703: FutureWarning: The Panel class is removed from pandas. Accessing it from the top-level namespace will also be removed in the next version\n",
      "  from pandas import Panel\n"
     ]
    }
   ],
   "source": [
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', None)\n",
    "#pd.set_option('display.max_colwidth', -1)\n",
    "decimal.getcontext().rounding = decimal.ROUND_DOWN\n",
    "logging.basicConfig(format='%(asctime)s : %(levelname)s : %(message)s', level=logging.INFO)\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# load movies.csv\n",
    "movies_df = pd.read_csv('generated_dataset/yago_pedia_ml_final_movies_list.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_df.replace('NOT_FOUND',np.nan, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def isNaN(string):\n",
    "    return string != string"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def data_processing(df, col_name):    \n",
    "    regex = \"\\(.*?\\)\"\n",
    "    df[col_name] = df[col_name].apply(lambda \n",
    "                            row: re.sub(regex,'',row) if isNaN(row)== False  else row)\n",
    "    df[col_name] = df[col_name].apply(lambda row: \n",
    "                            row.lower().strip().replace(' ','_').split('|') if isNaN(row)== False  else row)\n",
    "    df[col_name] = df[col_name].apply(lambda \n",
    "                            row: [item.strip() for item in row] if isNaN(row)== False  else row)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<bound method NDFrame.describe of                      movie_id                   title  \\\n",
       "0                       $9.99                   $9.99   \n",
       "1                  '68_(film)                     '68   \n",
       "2                  '71_(film)                     '71   \n",
       "3                 'Gator_Bait             'Gator Bait   \n",
       "4      'Neath_Brooklyn_Bridge  'Neath Brooklyn Bridge   \n",
       "...                       ...                     ...   \n",
       "36353                    Zozo                    Zozo   \n",
       "36354                Zubeidaa                Zubeidaa   \n",
       "36355        Zulu_(1964_film)                    Zulu   \n",
       "36356        Zulu_(2013_film)                    Zulu   \n",
       "36357                Zus_&_Zo                Zus & Zo   \n",
       "\n",
       "                                              actor_name  \\\n",
       "0                      [geoffrey_rush, anthony_lapaglia]   \n",
       "1                                [jan_němec, neil_young]   \n",
       "2      [richard_dormer, charlie_murphy_, sean_harris,...   \n",
       "3                                     [claudia_jennings]   \n",
       "4      [leo_gorcey, gabriel_dell, bobby_jordan, huntz...   \n",
       "...                                                  ...   \n",
       "36353                                    [carmen_lebbos]   \n",
       "36354  [rekha, karisma_kapoor, farida_jalal, amrish_p...   \n",
       "36355  [ulla_jacobsson, stanley_baker, james_booth, m...   \n",
       "36356                   [forest_whitaker, orlando_bloom]   \n",
       "36357      [monic_hendrickx, jacob_derwig, halina_reijn]   \n",
       "\n",
       "                        director_name                     writer_name  \\\n",
       "0      [etgar_keret, tatia_rosenthal]  [etgar_keret, tatia_rosenthal]   \n",
       "1                     [steven_kovacs]                 [steven_kovacs]   \n",
       "2                      [yann_demange]                 [gregory_burke]   \n",
       "3                 [beverly_sebastion]             [beverly_sebastion]   \n",
       "4                       [wallace_fox]                  [harvey_gates]   \n",
       "...                               ...                             ...   \n",
       "36353                   [josef_fares]                   [josef_fares]   \n",
       "36354                 [shyam_benegal]                [khalid_mohamed]   \n",
       "36355                   [cy_endfield]     [john_prebble, cy_endfield]   \n",
       "36356                  [jérôme_salle]                  [jérôme_salle]   \n",
       "36357            [paula_van_der_oest]            [paula_van_der_oest]   \n",
       "\n",
       "                                 country  \\\n",
       "0                    [israel, australia]   \n",
       "1                        [united_states]   \n",
       "2                       [united_kingdom]   \n",
       "3                        [united_states]   \n",
       "4                        [united_states]   \n",
       "...                                  ...   \n",
       "36353  [united_kingdom, sweden, lebanon]   \n",
       "36354                            [india]   \n",
       "36355                   [united_kingdom]   \n",
       "36356                           [france]   \n",
       "36357                      [netherlands]   \n",
       "\n",
       "                                                   genre  \n",
       "0                                                    NaN  \n",
       "1                          [lgbt-related_film_q20442589]  \n",
       "2      [crime_film, thriller_film, war_film, action_f...  \n",
       "3                           [thriller_film, action_film]  \n",
       "4                                                    NaN  \n",
       "...                                                  ...  \n",
       "36353                                                NaN  \n",
       "36354                                     [romance_film]  \n",
       "36355                         [adventure_film, war_film]  \n",
       "36356                                       [crime_film]  \n",
       "36357                                  [romantic_comedy]  \n",
       "\n",
       "[36358 rows x 7 columns]>"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df = data_processing(movies_df, 'actor_name')\n",
    "movies_df = data_processing(movies_df, 'writer_name')\n",
    "movies_df = data_processing(movies_df, 'genre')\n",
    "movies_df = data_processing(movies_df, 'country')\n",
    "movies_df = data_processing(movies_df, 'director_name')\n",
    "movies_df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>movie_id</th>\n",
       "      <th>title</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>$9.99</td>\n",
       "      <td>$9.99</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>'68_(film)</td>\n",
       "      <td>'68</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>'71_(film)</td>\n",
       "      <td>'71</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>'gator_bait</td>\n",
       "      <td>'gator bait</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>'neath_brooklyn_bridge</td>\n",
       "      <td>'neath brooklyn bridge</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36353</th>\n",
       "      <td>zozo</td>\n",
       "      <td>zozo</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36354</th>\n",
       "      <td>zubeidaa</td>\n",
       "      <td>zubeidaa</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36355</th>\n",
       "      <td>zulu_(1964_film)</td>\n",
       "      <td>zulu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36356</th>\n",
       "      <td>zulu_(2013_film)</td>\n",
       "      <td>zulu</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>36357</th>\n",
       "      <td>zus_&amp;_zo</td>\n",
       "      <td>zus &amp; zo</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>36358 rows × 2 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                     movie_id                   title\n",
       "0                       $9.99                   $9.99\n",
       "1                  '68_(film)                     '68\n",
       "2                  '71_(film)                     '71\n",
       "3                 'gator_bait             'gator bait\n",
       "4      'neath_brooklyn_bridge  'neath brooklyn bridge\n",
       "...                       ...                     ...\n",
       "36353                    zozo                    zozo\n",
       "36354                zubeidaa                zubeidaa\n",
       "36355        zulu_(1964_film)                    zulu\n",
       "36356        zulu_(2013_film)                    zulu\n",
       "36357                zus_&_zo                zus & zo\n",
       "\n",
       "[36358 rows x 2 columns]"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "movies_df['movie_id']=movies_df['movie_id'].str.lower()\n",
    "movies_df['title']=movies_df['title'].str.lower()\n",
    "movies_df[['movie_id','title']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'df' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-9-c4be14d5b018>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mregex\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m\"\\(.*?\\)\"\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[0mre\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msub\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mregex\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m''\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misNaN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m df[col_name] = df[col_name].apply(lambda row: \n\u001b[0;32m      4\u001b[0m                             row.lower().strip().replace(' ','_').split('|') if isNaN(row)== False  else row)\n\u001b[0;32m      5\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdf\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mcol_name\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mapply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m \u001b[1;33m[\u001b[0m\u001b[0mitem\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mstrip\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mitem\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m]\u001b[0m \u001b[1;32mif\u001b[0m \u001b[0misNaN\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m==\u001b[0m \u001b[1;32mFalse\u001b[0m  \u001b[1;32melse\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'df' is not defined"
     ]
    }
   ],
   "source": [
    "  \n",
    "regex = \"\\(.*?\\)\"\n",
    "df[col_name] = df[col_name].apply(lambda row: re.sub(regex,'',row) if isNaN(row)== False  else row)\n",
    "df[col_name] = df[col_name].apply(lambda row: \n",
    "                            row.lower().strip().replace(' ','_').split('|') if isNaN(row)== False  else row)\n",
    "df[col_name] = df[col_name].apply(lambda row: [item.strip() for item in row] if isNaN(row)== False  else row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create & Train doc2vec Model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Using movies from the dataset where non of the features is missing"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*** Choose the train data ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df =movies_df[~(movies_df.actor_name.isnull())\n",
    "                          & ~(movies_df.director_name.isnull())\n",
    "                          & ~(movies_df.writer_name.isnull())\n",
    "                          & ~(movies_df.genre.isnull())\n",
    "                          & ~(movies_df.country.isnull())]\n",
    "train_data_df.reset_index(drop= True, inplace=True)\n",
    "#train_data_df.to_csv(\"models/baseline_exp/train_data.csv\", index= False)\n",
    "#train_data_df.describe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Build movie sentence and create taggeddocuments ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_df.drop(movies_df.loc[movies_df.movie_id=='suburbia_(film)'].index, inplace= True)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_movie_sentence(actor,director,writer,genre,country):\n",
    "    return list(set(actor+director+writer+genre+country))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e5c897a89b9c4e308cc5b82514aa17d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=19492.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "<ipython-input-13-9fed7ca8e9c4>:1: SettingWithCopyWarning: \n",
      "A value is trying to be set on a copy of a slice from a DataFrame.\n",
      "Try using .loc[row_indexer,col_indexer] = value instead\n",
      "\n",
      "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
      "  train_data_df ['movie_sentence']= train_data_df.progress_apply(lambda row:  build_movie_sentence(row.actor_name, row.director_name, row.writer_name, row.genre, row.country), axis=1)\n"
     ]
    },
    {
     "ename": "FileNotFoundError",
     "evalue": "[Errno 2] No such file or directory: 'models/train_data/train_data.csv'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-13-9fed7ca8e9c4>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mtrain_data_df\u001b[0m \u001b[1;33m[\u001b[0m\u001b[1;34m'movie_sentence'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m=\u001b[0m \u001b[0mtrain_data_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprogress_apply\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;32mlambda\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m:\u001b[0m  \u001b[0mbuild_movie_sentence\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mactor_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdirector_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mwriter_name\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenre\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mrow\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcountry\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0maxis\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m1\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mtrain_data_df\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mto_csv\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"models/train_data/train_data.csv\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mindex\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\programs\\python39\\lib\\site-packages\\pandas\\core\\generic.py\u001b[0m in \u001b[0;36mto_csv\u001b[1;34m(self, path_or_buf, sep, na_rep, float_format, columns, header, index, index_label, mode, encoding, compression, quoting, quotechar, line_terminator, chunksize, date_format, doublequote, escapechar, decimal, errors)\u001b[0m\n\u001b[0;32m   3168\u001b[0m             \u001b[0mdecimal\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mdecimal\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3169\u001b[0m         )\n\u001b[1;32m-> 3170\u001b[1;33m         \u001b[0mformatter\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msave\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   3171\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   3172\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mpath_or_buf\u001b[0m \u001b[1;32mis\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python39\\lib\\site-packages\\pandas\\io\\formats\\csvs.py\u001b[0m in \u001b[0;36msave\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    183\u001b[0m             \u001b[0mclose\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mFalse\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    184\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 185\u001b[1;33m             f, handles = get_handle(\n\u001b[0m\u001b[0;32m    186\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    187\u001b[0m                 \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python39\\lib\\site-packages\\pandas\\io\\common.py\u001b[0m in \u001b[0;36mget_handle\u001b[1;34m(path_or_buf, mode, encoding, compression, memory_map, is_text, errors)\u001b[0m\n\u001b[0;32m    491\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    492\u001b[0m             \u001b[1;31m# Encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 493\u001b[1;33m             \u001b[0mf\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mopen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpath_or_buf\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmode\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mencoding\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0mencoding\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0merrors\u001b[0m\u001b[1;33m=\u001b[0m\u001b[0merrors\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mnewline\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;34m\"\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    494\u001b[0m         \u001b[1;32melif\u001b[0m \u001b[0mis_text\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    495\u001b[0m             \u001b[1;31m# No explicit encoding\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [Errno 2] No such file or directory: 'models/train_data/train_data.csv'"
     ]
    }
   ],
   "source": [
    "train_data_df ['movie_sentence']= train_data_df.progress_apply(lambda row:  build_movie_sentence(row.actor_name, row.director_name, row.writer_name, row.genre, row.country), axis=1)\n",
    "train_data_df.to_csv(\"models/train_data/train_data.csv\", index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\"'68_(film)\": \"'68\",\n",
       " \"'71_(film)\": \"'71\",\n",
       " \"'gator_bait\": \"'gator bait\",\n",
       " \"'neath_the_arizona_skies\": \"'neath the arizona skies\",\n",
       " \"'r_xmas\": \"'r xmas\",\n",
       " \"'til_there_was_you\": \"'til there was you\",\n",
       " \"'twas_the_night\": \"'twas the night\",\n",
       " \"'twas_the_night_before_christmas_(1977_tv_special)\": \"'twas the night before christmas\",\n",
       " '...ing': '...ing',\n",
       " '.45_(film)': '.45',\n",
       " '008:_operation_exterminate': '008: operation exterminate',\n",
       " '10,000_bc_(film)': '10,000 bc',\n",
       " '100%25_love_(2011_film)': '100% love',\n",
       " '1001_grams': '1001 grams',\n",
       " '100_bloody_acres': '100 bloody acres',\n",
       " '100_days_of_love': '100 days of love',\n",
       " '100_feet': '100 feet',\n",
       " '100_ghost_street:_the_return_of_richard_speck': '100 ghost street: the return of richard speck',\n",
       " '100_girls': '100 girls',\n",
       " '100_rifles': '100 rifles',\n",
       " '100_tears': '100 tears',\n",
       " '101_dalmatians_(1996_film)': '101 dalmatians',\n",
       " '102_dalmatians': '102 dalmatians',\n",
       " '10_(film)': '10',\n",
       " '10_cloverfield_lane': '10 cloverfield lane',\n",
       " '10_items_or_less_(film)': '10 items or less',\n",
       " '10_rillington_place': '10 rillington place',\n",
       " '10_rules_for_sleeping_around': '10 rules for sleeping around',\n",
       " '10_things_i_hate_about_you': '10 things i hate about you',\n",
       " '10_years_(2011_film)': '10 years',\n",
       " '10_to_midnight': '10 to midnight',\n",
       " '10th_&_wolf': '10th & wolf',\n",
       " '11-11-11_(film)': '11-11-11',\n",
       " '11.6': '11.6',\n",
       " '11:14': '11:14',\n",
       " '11_harrowhouse': '11 harrowhouse',\n",
       " '11_minutes_(film)': '11 minutes',\n",
       " '127_hours': '127 hours',\n",
       " '12_(2007_film)': '12',\n",
       " '12_angry_men_(1957_film)': '12 angry men',\n",
       " '12_dates_of_christmas': '12 dates of christmas',\n",
       " '12_rounds_(film)': '12 rounds',\n",
       " '12_years_a_slave_(film)': '12 years a slave',\n",
       " '12_to_the_moon': '12 to the moon',\n",
       " '13_(2010_film)': '13',\n",
       " '13_beloved': '13 beloved',\n",
       " '13_eerie': '13 eerie',\n",
       " '13_ghosts': '13 ghosts',\n",
       " '13_going_on_30': '13 going on 30',\n",
       " '13_minutes': '13 minutes',\n",
       " '13_moons': '13 moons',\n",
       " '13_rue_madeleine': '13 rue madeleine',\n",
       " '13_tzameti': '13 tzameti',\n",
       " '1408_(film)': '1408',\n",
       " '14_blades': '14 blades',\n",
       " '14_going_on_30': '14 going on 30',\n",
       " '150_milligrams': '150 milligrams',\n",
       " '15_minutes': '15 minutes',\n",
       " '16_blocks': '16 blocks',\n",
       " '1776_(film)': '1776',\n",
       " '17_again_(film)': '17 again',\n",
       " '18_again!': '18 again!',\n",
       " '1900_(film)': '1900',\n",
       " '1918_(1957_film)': '1918',\n",
       " '1920_(film)': '1920',\n",
       " '1941_(film)': '1941',\n",
       " '1944_(film)': '1944',\n",
       " '1969_(film)': '1969',\n",
       " '1981_(film)': '1981',\n",
       " '1984_(1956_film)': '1984',\n",
       " '1987_(film)': '1987',\n",
       " '1990:_the_bronx_warriors': '1990: the bronx warriors',\n",
       " '1:54_(film)': '1:54',\n",
       " '1_(2009_film)': '1',\n",
       " '1_(2013_film)': '1',\n",
       " '2.0_(film)': '2.0',\n",
       " '20,000_days_on_earth': '20,000 days on earth',\n",
       " '20,000_leagues_under_the_sea_(1916_film)': '20,000 leagues under the sea',\n",
       " '20,000_years_in_sing_sing': '20,000 years in sing sing',\n",
       " '2000_ad_(film)': '2000 ad',\n",
       " '2001:_a_space_odyssey_(film)': '2001: a space odyssey',\n",
       " '2001_maniacs': '2001 maniacs',\n",
       " '200_motels': '200 motels',\n",
       " '200_pounds_beauty': '200 pounds beauty',\n",
       " '2012:_ice_age': '2012: ice age',\n",
       " '2012_(film)': '2012',\n",
       " '2046_(film)': '2046',\n",
       " '20_mule_team': '20 mule team',\n",
       " '21_(2008_film)': '21',\n",
       " '21_grams': '21 grams',\n",
       " '21_jump_street_(film)': '21 jump street',\n",
       " '22_bullets': '22 bullets',\n",
       " '22_female_kottayam': '22 female kottayam',\n",
       " '22_jump_street': '22 jump street',\n",
       " '23_(film)': '23',\n",
       " '23_paces_to_baker_street': '23 paces to baker street',\n",
       " '24_(2016_film)': '24',\n",
       " '24_exposures': '24 exposures',\n",
       " '24_hour_party_people': '24 hour party people',\n",
       " '24_hours_(1931_film)': '24 hours',\n",
       " '25th_hour': '25th hour',\n",
       " '27_dresses': '27 dresses',\n",
       " '27_missing_kisses': '27 missing kisses',\n",
       " '28_days_(film)': '28 days',\n",
       " '28_days_later': '28 days later',\n",
       " '28_weeks_later': '28 weeks later',\n",
       " '29th_street_(film)': '29th street',\n",
       " '29th_and_gay': '29th and gay',\n",
       " '2ldk': '2ldk',\n",
       " '2_days_in_paris': '2 days in paris',\n",
       " '2_days_in_the_valley': '2 days in the valley',\n",
       " '2_fast_2_furious': '2 fast 2 furious',\n",
       " '2_guns': '2 guns',\n",
       " '2_seconds': '2 seconds',\n",
       " '3-headed_shark_attack': '3-headed shark attack',\n",
       " '3-iron': '3-iron',\n",
       " '3000_miles_to_graceland': '3000 miles to graceland',\n",
       " '30_beats': '30 beats',\n",
       " '30_days_of_night:_dark_days': '30 days of night: dark days',\n",
       " '30_days_of_night_(film)': '30 days of night',\n",
       " '30_is_a_dangerous_age,_cynthia': '30 is a dangerous age, cynthia',\n",
       " '30_nights_of_paranormal_activity_with_the_devil_inside_the_girl_with_the_dragon_tattoo': '30 nights of paranormal activity with the devil inside the girl with the dragon tattoo',\n",
       " '31_(film)': '31',\n",
       " '32a': '32a',\n",
       " '360_(film)': '360',\n",
       " '36_china_town': '36 china town',\n",
       " '36_chowringhee_lane': '36 chowringhee lane',\n",
       " '36_hours_(1953_film)': '36 hours',\n",
       " '36_hours_(1965_film)': '36 hours',\n",
       " '36_vayadhinile': '36 vayadhinile',\n",
       " '38_parrots': '38 parrots',\n",
       " '3:10_to_yuma_(2007_film)': '3:10 to yuma',\n",
       " '3_a.m._(2014_film)': '3 a.m.',\n",
       " '3_avengers': '3 avengers',\n",
       " '3_bad_men': '3 bad men',\n",
       " '3_days_to_kill': '3 days to kill',\n",
       " '3_idiots': '3 idiots',\n",
       " '3_men_in_white': '3 men in white',\n",
       " '3_nights_in_the_desert': '3 nights in the desert',\n",
       " '3_ninjas_(film)': '3 ninjas',\n",
       " '3_ninjas_kick_back': '3 ninjas kick back',\n",
       " '3_ninjas_knuckle_up': '3 ninjas knuckle up',\n",
       " '3_ring_circus': '3 ring circus',\n",
       " '3_strikes_(film)': '3 strikes',\n",
       " '400_days_(film)': '400 days',\n",
       " '40_days_and_40_nights': '40 days and 40 nights',\n",
       " '40_days_and_nights': '40 days and nights',\n",
       " '40_guns_to_apache_pass': '40 guns to apache pass',\n",
       " '42_(film)': '42',\n",
       " '42nd_street_(film)': '42nd street',\n",
       " '44_inch_chest': '44 inch chest',\n",
       " '47_ronin_(1994_film)': '47 ronin',\n",
       " '47_ronin_(2013_film)': '47 ronin',\n",
       " '48_hrs.': '48 hrs.',\n",
       " '49th_parallel_(film)': '49th parallel',\n",
       " '4:44_last_day_on_earth': '4:44 last day on earth',\n",
       " '4d_man': '4d man',\n",
       " '4_days_in_may': '4 days in may',\n",
       " '4_for_texas': '4 for texas',\n",
       " '4bia': '4bia',\n",
       " '4th_man_out': '4th man out',\n",
       " '4th_period_mystery': '4th period mystery',\n",
       " '50_first_dates': '50 first dates',\n",
       " \"5150_elm's_way\": \"5150 elm's way\",\n",
       " '51_(film)': '51',\n",
       " '52_pick-up': '52 pick-up',\n",
       " '54_(film)': '54',\n",
       " '55_days_at_peking': '55 days at peking',\n",
       " '5_against_the_house': '5 against the house',\n",
       " '5_card_stud': '5 card stud',\n",
       " '5_days_of_war': '5 days of war',\n",
       " '5_fingers': '5 fingers',\n",
       " '5_steps_to_danger': '5 steps to danger',\n",
       " '5_sundarikal': '5 sundarikal',\n",
       " '5_to_7': '5 to 7',\n",
       " '61*': '61*',\n",
       " '633_squadron': '633 squadron',\n",
       " '6_years': '6 years',\n",
       " '711_ocean_drive': '711 ocean drive',\n",
       " '71:_into_the_fire': '71: into the fire',\n",
       " '7g_rainbow_colony': '7g rainbow colony',\n",
       " '7_days_(film)': '7 days',\n",
       " '7_faces_of_dr._lao': '7 faces of dr. lao',\n",
       " '7_khoon_maaf': '7 khoon maaf',\n",
       " '7_minutes': '7 minutes',\n",
       " '800_bullets': '800 bullets',\n",
       " '80_minutes': '80 minutes',\n",
       " '88_minutes': '88 minutes',\n",
       " '8_heads_in_a_duffel_bag': '8 heads in a duffel bag',\n",
       " '8_mile_(film)': '8 mile',\n",
       " '8_million_ways_to_die': '8 million ways to die',\n",
       " '8_seconds': '8 seconds',\n",
       " '8_women': '8 women',\n",
       " '8mm_(film)': '8mm',\n",
       " '90_minutes_in_heaven_(film)': '90 minutes in heaven',\n",
       " '976-evil': '976-evil',\n",
       " '977_(film)': '977',\n",
       " '99_francs_(film)': '99 francs',\n",
       " '9_(2009_animated_film)': '9',\n",
       " '9_songs': '9 songs',\n",
       " 'a-haunting_we_will_go_(1942_film)': 'a-haunting we will go',\n",
       " 'a.i._artificial_intelligence': 'a.i. artificial intelligence',\n",
       " 'aka_(film)': 'aka',\n",
       " 'arq_(film)': 'arq',\n",
       " 'atm_(2012_film)': 'atm',\n",
       " 'a_(1998_kannada_film)': 'a',\n",
       " 'a_barefoot_dream': 'a barefoot dream',\n",
       " 'a_black_veil_for_lisa': 'a black veil for lisa',\n",
       " 'a_blade_in_the_dark': 'a blade in the dark',\n",
       " 'a_blood_pledge': 'a blood pledge',\n",
       " 'a_bloody_aria': 'a bloody aria',\n",
       " 'a_bomb_was_stolen': 'a bomb was stolen',\n",
       " 'a_boyfriend_for_christmas': 'a boyfriend for christmas',\n",
       " 'a_bucket_of_blood': 'a bucket of blood',\n",
       " 'a_bullet_for_pretty_boy': 'a bullet for pretty boy',\n",
       " 'a_chinese_ghost_story_iii': 'a chinese ghost story iii',\n",
       " 'a_christmas_carol_(1938_film)': 'a christmas carol',\n",
       " 'a_christmas_carol_(2000_film)': 'a christmas carol',\n",
       " 'a_christmas_carol_(2004_film)': 'a christmas carol',\n",
       " 'a_christmas_carol_(2009_film)': 'a christmas carol',\n",
       " 'a_christmas_kiss': 'a christmas kiss',\n",
       " 'a_christmas_romance': 'a christmas romance',\n",
       " 'a_christmas_visitor': 'a christmas visitor',\n",
       " 'a_cinderella_story:_if_the_shoe_fits': 'a cinderella story: if the shoe fits',\n",
       " 'a_cinderella_story:_once_upon_a_song': 'a cinderella story: once upon a song',\n",
       " 'a_company_man': 'a company man',\n",
       " \"a_connecticut_yankee_in_king_arthur's_court_(1921_film)\": \"a connecticut yankee in king arthur's court\",\n",
       " \"a_connecticut_yankee_in_king_arthur's_court_(1949_film)\": \"a connecticut yankee in king arthur's court\",\n",
       " 'a_crack_in_the_floor': 'a crack in the floor',\n",
       " 'a_cry_in_the_wild': 'a cry in the wild',\n",
       " 'a_cure_for_wellness': 'a cure for wellness',\n",
       " 'a_dandy_in_aspic': 'a dandy in aspic',\n",
       " 'a_day_in_the_death_of_joe_egg_(film)': 'a day in the death of joe egg',\n",
       " 'a_decent_man_(2015_swiss_film)': 'a decent man',\n",
       " 'a_different_loyalty': 'a different loyalty',\n",
       " 'a_different_story': 'a different story',\n",
       " 'a_distant_trumpet': 'a distant trumpet',\n",
       " \"a_dog's_purpose_(film)\": \"a dog's purpose\",\n",
       " 'a_dog_of_flanders_(1999_film)': 'a dog of flanders',\n",
       " \"a_doll's_house_(1959_film)\": \"a doll's house\",\n",
       " 'a_dream_of_kings_(film)': 'a dream of kings',\n",
       " 'a_family_affair_(1937_film)': 'a family affair',\n",
       " 'a_family_affair_(1984_film)': 'a family affair',\n",
       " \"a_fan's_notes_(film)\": \"a fan's notes\",\n",
       " \"a_fighter's_blues\": \"a fighter's blues\",\n",
       " 'a_florida_enchantment': 'a florida enchantment',\n",
       " 'a_foreign_affair': 'a foreign affair',\n",
       " 'a_frozen_flower': 'a frozen flower',\n",
       " 'a_game_of_death': 'a game of death',\n",
       " 'a_gathering_of_eagles': 'a gathering of eagles',\n",
       " 'a_girl_at_my_door': 'a girl at my door',\n",
       " 'a_girl_in_australia': 'a girl in australia',\n",
       " 'a_good_man_(2014_film)': 'a good man',\n",
       " 'a_good_marriage_(film)': 'a good marriage',\n",
       " 'a_great_wall': 'a great wall',\n",
       " 'a_grim_becoming': 'a grim becoming',\n",
       " 'a_gun_for_jennifer': 'a gun for jennifer',\n",
       " 'a_gunfight': 'a gunfight',\n",
       " 'a_hard_day': 'a hard day',\n",
       " 'a_hill_in_korea': 'a hill in korea',\n",
       " 'a_hole_in_the_head': 'a hole in the head',\n",
       " 'a_home_at_the_end_of_the_world_(film)': 'a home at the end of the world',\n",
       " 'a_hunting_accident': 'a hunting accident',\n",
       " 'a_kind_of_murder': 'a kind of murder',\n",
       " 'a_leading_man': 'a leading man',\n",
       " 'a_little_bit_zombie': 'a little bit zombie',\n",
       " 'a_little_night_music_(film)': 'a little night music',\n",
       " 'a_little_pond': 'a little pond',\n",
       " 'a_little_princess_(1995_film)': 'a little princess',\n",
       " 'a_long_ride_from_hell': 'a long ride from hell',\n",
       " 'a_man_betrayed_(1941_film)': 'a man betrayed',\n",
       " 'a_man_called_hero': 'a man called hero',\n",
       " 'a_man_called_magnum': 'a man called magnum',\n",
       " 'a_man_called_sarge': 'a man called sarge',\n",
       " 'a_man_could_get_killed': 'a man could get killed',\n",
       " 'a_man_named_john': 'a man named john',\n",
       " 'a_man_who_was_superman': 'a man who was superman',\n",
       " 'a_man_and_a_woman': 'a man and a woman',\n",
       " 'a_man_and_a_woman:_20_years_later': 'a man and a woman: 20 years later',\n",
       " 'a_matter_of_faith': 'a matter of faith',\n",
       " 'a_matter_of_resistance': 'a matter of resistance',\n",
       " 'a_matter_of_time_(film)': 'a matter of time',\n",
       " \"a_merry_friggin'_christmas\": \"a merry friggin' christmas\",\n",
       " \"a_midsummer_night's_dream_(1935_film)\": \"a midsummer night's dream\",\n",
       " \"a_midsummer_night's_dream_(1959_film)\": \"a midsummer night's dream\",\n",
       " \"a_midsummer_night's_dream_(1968_film)\": \"a midsummer night's dream\",\n",
       " \"a_midsummer_night's_dream_(1999_film)\": \"a midsummer night's dream\",\n",
       " 'a_million': 'a million',\n",
       " 'a_million_ways_to_die_in_the_west': 'a million ways to die in the west',\n",
       " \"a_millionaire's_first_love\": \"a millionaire's first love\",\n",
       " 'a_moment_of_romance': 'a moment of romance',\n",
       " 'a_mongolian_tale': 'a mongolian tale',\n",
       " 'a_monster_calls_(film)': 'a monster calls',\n",
       " 'a_most_violent_year': 'a most violent year',\n",
       " 'a_most_wanted_man_(film)': 'a most wanted man',\n",
       " 'a_new_life_(film)': 'a new life',\n",
       " 'a_night_in_the_woods': 'a night in the woods',\n",
       " 'a_nightingale_sang_in_berkeley_square_(film)': 'a nightingale sang in berkeley square',\n",
       " 'a_patch_of_fog': 'a patch of fog',\n",
       " 'a_perfect_day_(2015_film)': 'a perfect day',\n",
       " 'a_pistol_for_ringo': 'a pistol for ringo',\n",
       " 'a_prize_of_gold': 'a prize of gold',\n",
       " 'a_promise_(2013_film)': 'a promise',\n",
       " 'a_quiet_passion': 'a quiet passion',\n",
       " 'a_quiet_place_in_the_country': 'a quiet place in the country',\n",
       " 'a_quiet_place_to_kill': 'a quiet place to kill',\n",
       " 'a_reason_to_live,_a_reason_to_die': 'a reason to live, a reason to die',\n",
       " 'a_reflection_of_fear': 'a reflection of fear',\n",
       " 'a_royal_christmas': 'a royal christmas',\n",
       " 'a_royal_night_out': 'a royal night out',\n",
       " 'a_royal_scandal_(film)': 'a royal scandal',\n",
       " 'a_scandal_in_paris': 'a scandal in paris',\n",
       " 'a_second_chance_(2014_film)': 'a second chance',\n",
       " 'a_separate_peace_(film)': 'a separate peace',\n",
       " 'a_shriek_in_the_night': 'a shriek in the night',\n",
       " 'a_simple_story_(1960_film)': 'a simple story',\n",
       " \"a_soldier's_story\": \"a soldier's story\",\n",
       " 'a_special_day': 'a special day',\n",
       " 'a_star_is_born_(1937_film)': 'a star is born',\n",
       " 'a_star_is_born_(1954_film)': 'a star is born',\n",
       " 'a_star_is_born_(1976_film)': 'a star is born',\n",
       " 'a_stranger_is_watching_(film)': 'a stranger is watching',\n",
       " 'a_stranger_of_mine': 'a stranger of mine',\n",
       " 'a_suitcase_for_a_corpse': 'a suitcase for a corpse',\n",
       " 'a_summer_story': 'a summer story',\n",
       " 'a_tale_of_legendary_libido': 'a tale of legendary libido',\n",
       " 'a_tale_of_love_and_darkness_(film)': 'a tale of love and darkness',\n",
       " 'a_tale_of_sorrow_and_sadness': 'a tale of sorrow and sadness',\n",
       " 'a_tale_of_two_cities_(1935_film)': 'a tale of two cities',\n",
       " 'a_tale_of_two_cities_(1958_film)': 'a tale of two cities',\n",
       " 'a_talking_cat!%3f!': 'a talking cat!?!',\n",
       " 'a_thief_in_the_night_(film)': 'a thief in the night',\n",
       " 'a_thunder_of_drums': 'a thunder of drums',\n",
       " 'a_ticket_to_tomahawk': 'a ticket to tomahawk',\n",
       " 'a_time_for_dying': 'a time for dying',\n",
       " 'a_town_without_christmas': 'a town without christmas',\n",
       " 'a_tree_of_palme': 'a tree of palme',\n",
       " 'a_trick_of_light': 'a trick of light',\n",
       " 'a_trip_to_the_moon': 'a trip to the moon',\n",
       " 'a_true_mob_story': 'a true mob story',\n",
       " 'a_united_kingdom': 'a united kingdom',\n",
       " 'a_very_merry_mix-up': 'a very merry mix-up',\n",
       " 'a_very_murray_christmas': 'a very murray christmas',\n",
       " 'a_very_special_favor': 'a very special favor',\n",
       " 'a_violent_prosecutor': 'a violent prosecutor',\n",
       " 'a_virgin_among_the_living_dead': 'a virgin among the living dead',\n",
       " 'a_walk_in_the_spring_rain': 'a walk in the spring rain',\n",
       " 'a_walk_in_the_sun_(1945_film)': 'a walk in the sun',\n",
       " 'a_walk_in_the_woods_(film)': 'a walk in the woods',\n",
       " \"a_woman's_devotion\": \"a woman's devotion\",\n",
       " \"a_woman's_vengeance\": \"a woman's vengeance\",\n",
       " 'a_woman_at_her_window': 'a woman at her window',\n",
       " 'a_woman_of_experience': 'a woman of experience',\n",
       " 'a_yank_at_oxford': 'a yank at oxford',\n",
       " 'a_year_of_the_quiet_sun': 'a year of the quiet sun',\n",
       " 'a_la_mala': 'a la mala',\n",
       " 'aaaaaaaah!': 'aaaaaaaah!',\n",
       " 'aadhavan': 'aadhavan',\n",
       " 'aadukalam': 'aadukalam',\n",
       " 'aaja_nachle': 'aaja nachle',\n",
       " 'aakrosh_(2010_film)': 'aakrosh',\n",
       " 'aalavandhan': 'aalavandhan',\n",
       " 'aamir_(film)': 'aamir',\n",
       " 'aankhen_(1993_film)': 'aankhen',\n",
       " 'aankhen_(2002_film)': 'aankhen',\n",
       " 'aap_mujhe_achche_lagne_lage': 'aap mujhe achche lagne lage',\n",
       " 'aarakshan': 'aarakshan',\n",
       " 'aaron_loves_angela': 'aaron loves angela',\n",
       " 'aasai': 'aasai',\n",
       " 'aashiq_(2001_film)': 'aashiq',\n",
       " 'aashiqui_2': 'aashiqui 2',\n",
       " 'aayirathil_oruvan_(1965_film)': 'aayirathil oruvan',\n",
       " 'aayirathil_oruvan_(2010_film)': 'aayirathil oruvan',\n",
       " 'ab-normal_beauty': 'ab-normal beauty',\n",
       " 'abandon_(film)': 'abandon',\n",
       " 'abandoned_(1949_film)': 'abandoned',\n",
       " 'abandoned_(2010_film)': 'abandoned',\n",
       " 'abattoir_(film)': 'abattoir',\n",
       " 'abbott_and_costello_go_to_mars': 'abbott and costello go to mars',\n",
       " 'abbott_and_costello_meet_captain_kidd': 'abbott and costello meet captain kidd',\n",
       " 'abbott_and_costello_meet_dr._jekyll_and_mr._hyde': 'abbott and costello meet dr. jekyll and mr. hyde',\n",
       " 'abbott_and_costello_meet_the_invisible_man': 'abbott and costello meet the invisible man',\n",
       " 'abbott_and_costello_meet_the_keystone_kops': 'abbott and costello meet the keystone kops',\n",
       " 'abbott_and_costello_meet_the_killer,_boris_karloff': 'abbott and costello meet the killer, boris karloff',\n",
       " 'abbott_and_costello_meet_the_mummy': 'abbott and costello meet the mummy',\n",
       " 'abby_(film)': 'abby',\n",
       " 'abducted:_the_carlina_white_story': 'abducted: the carlina white story',\n",
       " 'abduction_(2011_film)': 'abduction',\n",
       " 'abe_lincoln_in_illinois_(film)': 'abe lincoln in illinois',\n",
       " 'about_adam': 'about adam',\n",
       " 'about_face_(1952_film)': 'about face',\n",
       " 'about_last_night_(2014_film)': 'about last night',\n",
       " 'about_schmidt': 'about schmidt',\n",
       " 'about_time_(2013_film)': 'about time',\n",
       " 'about_a_boy_(film)': 'about a boy',\n",
       " 'above_suspicion_(1943_film)': 'above suspicion',\n",
       " 'above_us_the_waves': 'above us the waves',\n",
       " 'above_the_rim': 'above the rim',\n",
       " 'abraham_(film)': 'abraham',\n",
       " 'abraham_lincoln_(1930_film)': 'abraham lincoln',\n",
       " 'abraham_lincoln_vs._zombies': 'abraham lincoln vs. zombies',\n",
       " 'abraxas,_guardian_of_the_universe': 'abraxas, guardian of the universe',\n",
       " 'absence_of_malice': 'absence of malice',\n",
       " 'absence_of_the_good': 'absence of the good',\n",
       " 'absentia_(film)': 'absentia',\n",
       " 'absolute_beginners_(film)': 'absolute beginners',\n",
       " 'absolute_giganten': 'absolute giganten',\n",
       " 'absolute_power_(film)': 'absolute power',\n",
       " 'absolute_zero_(film)': 'absolute zero',\n",
       " 'absolutely_anything': 'absolutely anything',\n",
       " 'absolution_(1978_film)': 'absolution',\n",
       " 'absolution_(2015_film)': 'absolution',\n",
       " 'abzurdah': 'abzurdah',\n",
       " 'accident_(1928_film)': 'accident',\n",
       " 'accident_(2008_film)': 'accident',\n",
       " 'accident_(2009_film)': 'accident',\n",
       " 'accomplice_(film)': 'accomplice',\n",
       " 'accomplices_(film)': 'accomplices',\n",
       " 'according_to_greta': 'according to greta',\n",
       " 'according_to_spencer': 'according to spencer',\n",
       " 'accumulator_1': 'accumulator 1',\n",
       " 'accused_(1936_film)': 'accused',\n",
       " 'ace_attorney_(film)': 'ace attorney',\n",
       " 'ace_high_(1919_film)': 'ace high',\n",
       " 'ace_high_(1968_film)': 'ace high',\n",
       " 'ace_in_the_hole_(1951_film)': 'ace in the hole',\n",
       " 'ace_of_aces_(1933_film)': 'ace of aces',\n",
       " 'aces_high_(film)': 'aces high',\n",
       " 'achilles_and_the_tortoise_(film)': 'achilles and the tortoise',\n",
       " 'acolytes_(film)': 'acolytes',\n",
       " 'across_110th_street': 'across 110th street',\n",
       " 'across_the_bridge_(film)': 'across the bridge',\n",
       " 'across_the_great_divide_(film)': 'across the great divide',\n",
       " 'across_the_line:_the_exodus_of_charlie_wright': 'across the line: the exodus of charlie wright',\n",
       " 'across_the_pacific_(1926_film)': 'across the pacific',\n",
       " 'across_the_tracks': 'across the tracks',\n",
       " 'across_the_universe_(film)': 'across the universe',\n",
       " 'across_to_singapore': 'across to singapore',\n",
       " 'act_of_love_(1953_film)': 'act of love',\n",
       " 'act_of_violence': 'act of violence',\n",
       " 'action_(1921_film)': 'action',\n",
       " 'action_jackson_(1988_film)': 'action jackson',\n",
       " 'action_jackson_(2014_film)': 'action jackson',\n",
       " 'action_replayy': 'action replayy',\n",
       " 'action_in_arabia': 'action in arabia',\n",
       " 'action_in_the_north_atlantic': 'action in the north atlantic',\n",
       " \"adam's_rib\": \"adam's rib\",\n",
       " 'adam_&_eva': 'adam & eva',\n",
       " 'adam_&_paul': 'adam & paul',\n",
       " 'adam_&_steve': 'adam & steve',\n",
       " 'adam_(2009_film)': 'adam',\n",
       " \"adam_green's_aladdin\": \"adam green's aladdin\",\n",
       " 'adam_had_four_sons': 'adam had four sons',\n",
       " 'addams_family_values': 'addams family values',\n",
       " 'addicted_(2002_film)': 'addicted',\n",
       " 'addicted_to_fresno': 'addicted to fresno',\n",
       " 'addicted_to_love_(film)': 'addicted to love',\n",
       " 'adhurs': 'adhurs',\n",
       " 'adieu_bonaparte': 'adieu bonaparte',\n",
       " 'aditya_369': 'aditya 369',\n",
       " 'adrenaline_drive': 'adrenaline drive',\n",
       " 'adult_world': 'adult world',\n",
       " 'adulthood_(film)': 'adulthood',\n",
       " 'advance_to_the_rear': 'advance to the rear',\n",
       " 'advantageous': 'advantageous',\n",
       " 'adventure_(1925_film)': 'adventure',\n",
       " 'adventure_(1945_film)': 'adventure',\n",
       " 'adventureland_(film)': 'adventureland',\n",
       " 'adventures_in_babysitting': 'adventures in babysitting',\n",
       " 'adventures_in_babysitting_(2016_film)': 'adventures in babysitting',\n",
       " 'adventures_of_captain_fabian': 'adventures of captain fabian',\n",
       " 'adventures_of_don_juan': 'adventures of don juan',\n",
       " \"adventures_of_kitty_o'day\": \"adventures of kitty o'day\",\n",
       " 'adventures_of_zatoichi': 'adventures of zatoichi',\n",
       " 'aegan': 'aegan',\n",
       " 'aenigma_(film)': 'aenigma',\n",
       " 'aerial_gunner': 'aerial gunner',\n",
       " 'aferim!': 'aferim!',\n",
       " 'affair_in_havana': 'affair in havana',\n",
       " 'affair_in_trinidad': 'affair in trinidad',\n",
       " 'affectionately_yours': 'affectionately yours',\n",
       " 'affinity_(film)': 'affinity',\n",
       " 'afonya': 'afonya',\n",
       " 'after..._(film)': 'after...',\n",
       " 'after.life': 'after.life',\n",
       " 'after_(2012_film)': 'after',\n",
       " 'after_dark,_my_sweet': 'after dark, my sweet',\n",
       " 'after_earth': 'after earth',\n",
       " 'after_life_(film)': 'after life',\n",
       " 'after_office_hours': 'after office hours',\n",
       " 'after_office_hours_(1932_film)': 'after office hours',\n",
       " 'after_sex_(2000_film)': 'after sex',\n",
       " 'after_sex_(2007_film)': 'after sex',\n",
       " 'after_tonight': 'after tonight',\n",
       " 'after_the_ball_(1932_film)': 'after the ball',\n",
       " 'after_the_ball_(1957_film)': 'after the ball',\n",
       " 'after_the_fox': 'after the fox',\n",
       " 'after_the_sunset': 'after the sunset',\n",
       " 'after_the_thin_man': 'after the thin man',\n",
       " 'aftershock_(1990_film)': 'aftershock',\n",
       " 'aftershock_(2012_film)': 'aftershock',\n",
       " 'against_all_flags': 'against all flags',\n",
       " 'against_the_law_(1950_film)': 'against the law',\n",
       " 'against_the_law_(1997_film)': 'against the law',\n",
       " 'against_the_sun': 'against the sun',\n",
       " 'against_the_wind_(film)': 'against the wind',\n",
       " 'agatha_(film)': 'agatha',\n",
       " 'age_of_panic': 'age of panic',\n",
       " 'age_of_the_dragons': 'age of the dragons',\n",
       " 'agent_cody_banks': 'agent cody banks',\n",
       " 'agent_vinod_(1977_film)': 'agent vinod',\n",
       " 'agent_vinod_(2012_film)': 'agent vinod',\n",
       " 'agent_for_h.a.r.m.': 'agent for h.a.r.m.',\n",
       " 'agnes_of_god_(film)': 'agnes of god',\n",
       " 'agni_natchathiram': 'agni natchathiram',\n",
       " 'agni_sakshi_(1996_film)': 'agni sakshi',\n",
       " 'agony_(film)': 'agony',\n",
       " 'agora_(film)': 'agora',\n",
       " 'ah_boys_to_men': 'ah boys to men',\n",
       " 'aileen:_life_and_death_of_a_serial_killer': 'aileen: life and death of a serial killer',\n",
       " 'aimy_in_a_cage': 'aimy in a cage',\n",
       " \"ain't_them_bodies_saints\": \"ain't them bodies saints\",\n",
       " 'air_(2005_film)': 'air',\n",
       " 'air_(2015_film)': 'air',\n",
       " 'air_america_(film)': 'air america',\n",
       " 'air_bud': 'air bud',\n",
       " 'air_buddies': 'air buddies',\n",
       " 'air_crew': 'air crew',\n",
       " 'air_force_(film)': 'air force',\n",
       " 'air_force_one_(film)': 'air force one',\n",
       " 'air_hawks': 'air hawks',\n",
       " 'air_raid_wardens': 'air raid wardens',\n",
       " 'airbag_(film)': 'airbag',\n",
       " 'airborne_(2012_film)': 'airborne',\n",
       " 'airheads': 'airheads',\n",
       " 'airlift_(film)': 'airlift',\n",
       " 'airplane_ii:_the_sequel': 'airplane ii: the sequel',\n",
       " 'airport_(1970_film)': 'airport',\n",
       " 'airport_(1993_film)': 'airport',\n",
       " 'airport_1975': 'airport 1975',\n",
       " 'aitraaz': 'aitraaz',\n",
       " 'aiyyaa': 'aiyyaa',\n",
       " 'ajab_prem_ki_ghazab_kahani': 'ajab prem ki ghazab kahani',\n",
       " 'ajnabee_(2001_film)': 'ajnabee',\n",
       " 'akeelah_and_the_bee': 'akeelah and the bee',\n",
       " 'akira_(1988_film)': 'akira',\n",
       " 'akira_(2016_hindi_film)': 'akira',\n",
       " 'al_capone_(film)': 'al capone',\n",
       " 'alabama_moon_(film)': 'alabama moon',\n",
       " 'aladdin_(1992_disney_film)': 'aladdin',\n",
       " 'aladin_(film)': 'aladin',\n",
       " 'alarm_(1938_film)': 'alarm',\n",
       " 'alaska_(1944_film)': 'alaska',\n",
       " 'alaska_(1996_film)': 'alaska',\n",
       " 'alatriste': 'alatriste',\n",
       " 'albert_nobbs': 'albert nobbs',\n",
       " 'albino_(film)': 'albino',\n",
       " 'albino_alligator': 'albino alligator',\n",
       " \"aleksandr's_price\": \"aleksandr's price\",\n",
       " 'alex_&_eve': 'alex & eve',\n",
       " 'alex_cross_(film)': 'alex cross',\n",
       " 'alex_in_wonderland': 'alex in wonderland',\n",
       " \"alexander's_ragtime_band_(film)\": \"alexander's ragtime band\",\n",
       " 'alexander_(2004_film)': 'alexander',\n",
       " 'alexander_hamilton_(film)': 'alexander hamilton',\n",
       " 'alexander_nevsky_(film)': 'alexander nevsky',\n",
       " 'alexander_the_great_(1956_film)': 'alexander the great',\n",
       " 'alexandra_(film)': 'alexandra',\n",
       " 'alexandria..._why%3f': 'alexandria... why?',\n",
       " 'alfie_(1966_film)': 'alfie',\n",
       " 'alfie_(2004_film)': 'alfie',\n",
       " 'alfred_the_great_(film)': 'alfred the great',\n",
       " 'algiers_(film)': 'algiers',\n",
       " 'ali:_fear_eats_the_soul': 'ali: fear eats the soul',\n",
       " 'ali_(film)': 'ali',\n",
       " 'ali_baba_goes_to_town': 'ali baba goes to town',\n",
       " 'ali_g_indahouse': 'ali g indahouse',\n",
       " 'alias_boston_blackie': 'alias boston blackie',\n",
       " 'alias_jesse_james': 'alias jesse james',\n",
       " 'alias_john_preston': 'alias john preston',\n",
       " 'alias_nick_beal': 'alias nick beal',\n",
       " 'alibi_(1929_film)': 'alibi',\n",
       " 'alibi_(1942_film)': 'alibi',\n",
       " 'alibi_(2007_film)': 'alibi',\n",
       " \"alice's_adventures_in_wonderland_(1972_film)\": \"alice's adventures in wonderland\",\n",
       " \"alice's_restaurant_(film)\": \"alice's restaurant\",\n",
       " 'alice,_sweet_alice': 'alice, sweet alice',\n",
       " 'alice_(1988_film)': 'alice',\n",
       " 'alice_(1990_film)': 'alice',\n",
       " \"alice_doesn't_live_here_anymore\": \"alice doesn't live here anymore\",\n",
       " 'alice_through_the_looking_glass_(2016_film)': 'alice through the looking glass',\n",
       " 'alice_in_wonderland_(1903_film)': 'alice in wonderland',\n",
       " 'alice_in_wonderland_(1931_film)': 'alice in wonderland',\n",
       " 'alice_in_wonderland_(1933_film)': 'alice in wonderland',\n",
       " 'alice_in_wonderland_(1949_film)': 'alice in wonderland',\n",
       " 'alice_in_wonderland_(2010_film)': 'alice in wonderland',\n",
       " 'alien:_covenant': 'alien: covenant',\n",
       " 'alien_(film)': 'alien',\n",
       " 'alien_autopsy_(film)': 'alien autopsy',\n",
       " 'alien_express': 'alien express',\n",
       " 'alien_hunter': 'alien hunter',\n",
       " 'alien_lockdown': 'alien lockdown',\n",
       " 'alien_nation_(film)': 'alien nation',\n",
       " 'alien_siege': 'alien siege',\n",
       " 'alien_from_l.a.': 'alien from l.a.',\n",
       " 'alienator': 'alienator',\n",
       " 'aliens_(film)': 'aliens',\n",
       " 'aliens_in_the_attic': 'aliens in the attic',\n",
       " 'aligarh_(film)': 'aligarh',\n",
       " 'alita:_battle_angel': 'alita: battle angel',\n",
       " 'alive_(1993_film)': 'alive',\n",
       " 'alive_(2002_film)': 'alive',\n",
       " 'alive_in_joburg': 'alive in joburg',\n",
       " 'all_about_eve': 'all about eve',\n",
       " 'all_about_evil': 'all about evil',\n",
       " 'all_about_love_(2006_film)': 'all about love',\n",
       " 'all_about_my_mother': 'all about my mother',\n",
       " 'all_about_my_wife': 'all about my wife',\n",
       " 'all_about_steve': 'all about steve',\n",
       " 'all_about_them': 'all about them',\n",
       " 'all_about_the_benjamins': 'all about the benjamins',\n",
       " 'all_cheerleaders_die': 'all cheerleaders die',\n",
       " 'all_dogs_go_to_heaven': 'all dogs go to heaven',\n",
       " 'all_eyez_on_me_(film)': 'all eyez on me',\n",
       " 'all_good_things_(film)': 'all good things',\n",
       " 'all_hands_on_deck_(film)': 'all hands on deck',\n",
       " 'all_i_see_is_you_(film)': 'all i see is you',\n",
       " 'all_i_want_(film)': 'all i want',\n",
       " 'all_i_want_for_christmas_(film)': 'all i want for christmas',\n",
       " 'all_in_(film)': 'all in',\n",
       " 'all_is_bright': 'all is bright',\n",
       " 'all_is_lost': 'all is lost',\n",
       " 'all_men_are_brothers_(film)': 'all men are brothers',\n",
       " 'all_mine_to_give': 'all mine to give',\n",
       " 'all_my_friends_are_leaving_brisbane': 'all my friends are leaving brisbane',\n",
       " 'all_night_long_(1962_film)': 'all night long',\n",
       " 'all_night_long_(1992_film)': 'all night long',\n",
       " 'all_over_me_(film)': 'all over me',\n",
       " 'all_over_the_guy': 'all over the guy',\n",
       " 'all_quiet_on_the_western_front_(1930_film)': 'all quiet on the western front',\n",
       " 'all_that_heaven_allows': 'all that heaven allows',\n",
       " 'all_that_jazz_(film)': 'all that jazz',\n",
       " 'all_things_fair': 'all things fair',\n",
       " 'all_this,_and_heaven_too': 'all this, and heaven too',\n",
       " 'all_through_the_night_(film)': 'all through the night',\n",
       " \"all_tomorrow's_parties_(2003_film)\": \"all tomorrow's parties\",\n",
       " 'all_you_need_is_love_(film)': 'all you need is love',\n",
       " \"all_in_a_night's_work_(film)\": \"all in a night's work\",\n",
       " 'all_or_nothing_(film)': 'all or nothing',\n",
       " 'all_the_boys_love_mandy_lane': 'all the boys love mandy lane',\n",
       " 'all_the_brothers_were_valiant': 'all the brothers were valiant',\n",
       " 'all_the_brothers_were_valiant_(1923_film)': 'all the brothers were valiant',\n",
       " 'all_the_days_before_tomorrow': 'all the days before tomorrow',\n",
       " 'all_the_fine_young_cannibals': 'all the fine young cannibals',\n",
       " \"all_the_king's_men_(1949_film)\": \"all the king's men\",\n",
       " \"all_the_king's_men_(2006_film)\": \"all the king's men\",\n",
       " 'all_the_light_in_the_sky': 'all the light in the sky',\n",
       " \"all_the_president's_men_(film)\": \"all the president's men\",\n",
       " 'all_the_pretty_horses_(film)': 'all the pretty horses',\n",
       " \"all_the_queen's_men\": \"all the queen's men\",\n",
       " 'all_the_real_girls': 'all the real girls',\n",
       " 'all_the_right_moves_(film)': 'all the right moves',\n",
       " 'all_the_vermeers_in_new_york': 'all the vermeers in new york',\n",
       " 'all_the_wrong_reasons_(film)': 'all the wrong reasons',\n",
       " 'all_the_young_men': 'all the young men',\n",
       " 'alla_mia_cara_mamma_nel_giorno_del_suo_compleanno': 'alla mia cara mamma nel giorno del suo compleanno',\n",
       " 'allegheny_uprising': 'allegheny uprising',\n",
       " 'allegro_(film)': 'allegro',\n",
       " 'allegro_non_troppo': 'allegro non troppo',\n",
       " \"alleluia!_the_devil's_carnival\": \"alleluia! the devil's carnival\",\n",
       " 'allied_(film)': 'allied',\n",
       " 'alligator_(film)': 'alligator',\n",
       " 'allotment_wives': 'allotment wives',\n",
       " 'almost_christmas_(film)': 'almost christmas',\n",
       " 'almost_famous': 'almost famous',\n",
       " 'almost_human_(1974_film)': 'almost human',\n",
       " 'almost_married_(1932_film)': 'almost married',\n",
       " 'aloha_summer': 'aloha summer',\n",
       " 'alone_(2007_film)': 'alone',\n",
       " 'alone_(2008_film)': 'alone',\n",
       " 'alone_in_the_dark_(1982_film)': 'alone in the dark',\n",
       " 'alone_in_the_dark_(2005_film)': 'alone in the dark',\n",
       " 'alone_in_the_dark_ii_(film)': 'alone in the dark ii',\n",
       " 'along_came_jones_(film)': 'along came jones',\n",
       " 'along_came_polly': 'along came polly',\n",
       " 'along_came_a_spider_(film)': 'along came a spider',\n",
       " 'along_the_great_divide': 'along the great divide',\n",
       " 'alpha_dog': 'alpha dog',\n",
       " 'alphabet_city_(film)': 'alphabet city',\n",
       " 'alphaville_(film)': 'alphaville',\n",
       " 'alraune_(1918_film)': 'alraune',\n",
       " 'alraune_(1928_film)': 'alraune',\n",
       " 'alraune_(1930_film)': 'alraune',\n",
       " 'altered_states': 'altered states',\n",
       " 'altergeist': 'altergeist',\n",
       " 'alucarda': 'alucarda',\n",
       " 'alvarez_kelly': 'alvarez kelly',\n",
       " 'alvin_and_the_chipmunks:_chipwrecked': 'alvin and the chipmunks: chipwrecked',\n",
       " 'always_(1989_film)': 'always',\n",
       " 'always_(2011_film)': 'always',\n",
       " 'always_be_my_maybe': 'always be my maybe',\n",
       " 'always_shine': 'always shine',\n",
       " 'always_a_bride': 'always a bride',\n",
       " 'amadeus_(film)': 'amadeus',\n",
       " 'amanda_knox:_murder_on_trial_in_italy': 'amanda knox: murder on trial in italy',\n",
       " 'amar_akbar_anthony_(2015_film)': 'amar akbar anthony',\n",
       " 'amarcord': 'amarcord',\n",
       " 'amarilly_of_clothes-line_alley': 'amarilly of clothes-line alley',\n",
       " 'amarkalam': 'amarkalam',\n",
       " 'amateur_night_(2016_film)': 'amateur night',\n",
       " 'amazing_grace_(1992_film)': 'amazing grace',\n",
       " 'amazing_grace_(2006_film)': 'amazing grace',\n",
       " 'amazon_women_on_the_moon': 'amazon women on the moon',\n",
       " 'amazons_(1986_film)': 'amazons',\n",
       " 'ambush_(1939_film)': 'ambush',\n",
       " 'ambush_(1950_film)': 'ambush',\n",
       " 'ambush_(1999_film)': 'ambush',\n",
       " 'ambush_at_cimarron_pass': 'ambush at cimarron pass',\n",
       " 'amelia_(film)': 'amelia',\n",
       " 'amelia_earhart:_the_final_flight': 'amelia earhart: the final flight',\n",
       " 'amen.': 'amen.',\n",
       " 'amer_(film)': 'amer',\n",
       " \"america's_sweethearts\": \"america's sweethearts\",\n",
       " 'america_(1924_film)': 'america',\n",
       " 'american_beauty_(1927_film)': 'american beauty',\n",
       " 'american_beauty_(1999_film)': 'american beauty',\n",
       " 'american_buffalo_(film)': 'american buffalo',\n",
       " 'american_burger': 'american burger',\n",
       " 'american_dreamz': 'american dreamz',\n",
       " 'american_fable': 'american fable',\n",
       " 'american_gangster_(film)': 'american gangster',\n",
       " 'american_gigolo': 'american gigolo',\n",
       " 'american_guerrilla_in_the_philippines': 'american guerrilla in the philippines',\n",
       " 'american_heist': 'american heist',\n",
       " 'american_hero_(film)': 'american hero',\n",
       " 'american_high_school_(film)': 'american high school',\n",
       " 'american_history_x': 'american history x',\n",
       " 'american_honey_(film)': 'american honey',\n",
       " 'american_hot_wax': 'american hot wax',\n",
       " 'american_hustle': 'american hustle',\n",
       " 'american_made_(film)': 'american made',\n",
       " 'american_mary': 'american mary',\n",
       " 'american_me': 'american me',\n",
       " 'american_nightmare_(film)': 'american nightmare',\n",
       " 'american_ninja_2:_the_confrontation': 'american ninja 2: the confrontation',\n",
       " 'american_ninja_3:_blood_hunt': 'american ninja 3: blood hunt',\n",
       " 'american_outlaws': 'american outlaws',\n",
       " 'american_pastime_(film)': 'american pastime',\n",
       " 'american_perfekt': 'american perfekt',\n",
       " 'american_pie_(film)': 'american pie',\n",
       " 'american_pie_2': 'american pie 2',\n",
       " 'american_pop': 'american pop',\n",
       " 'american_psycho_(film)': 'american psycho',\n",
       " 'american_roulette_(film)': 'american roulette',\n",
       " 'american_shaolin': 'american shaolin',\n",
       " 'american_sharia': 'american sharia',\n",
       " 'american_sniper': 'american sniper',\n",
       " 'american_splendor_(film)': 'american splendor',\n",
       " 'american_strays': 'american strays',\n",
       " 'american_ultra': 'american ultra',\n",
       " 'american_violet': 'american violet',\n",
       " 'american_warships': 'american warships',\n",
       " 'american_wedding': 'american wedding',\n",
       " 'american_zombie': 'american zombie',\n",
       " 'amistad_(film)': 'amistad',\n",
       " 'amityville:_a_new_generation': 'amityville: a new generation',\n",
       " 'amityville:_vanishing_point': 'amityville: vanishing point',\n",
       " 'amityville_3-d': 'amityville 3-d',\n",
       " 'amityville_ii:_the_possession': 'amityville ii: the possession',\n",
       " 'amnesiac_(film)': 'amnesiac',\n",
       " 'amok_(1944_film)': 'amok',\n",
       " 'among_the_living_(1941_film)': 'among the living',\n",
       " 'among_the_living_(2014_film)': 'among the living',\n",
       " 'amore_libero_-_free_love': 'amore libero - free love',\n",
       " 'amores_perros': 'amores perros',\n",
       " 'amos_&_andrew': 'amos & andrew',\n",
       " 'amphetamine_(film)': 'amphetamine',\n",
       " 'amphibian_man_(film)': 'amphibian man',\n",
       " 'amsterdam_heavy': 'amsterdam heavy',\n",
       " 'amsterdamned': 'amsterdamned',\n",
       " 'amuck!': 'amuck!',\n",
       " 'amusement_(film)': 'amusement',\n",
       " 'an_alligator_named_daisy': 'an alligator named daisy',\n",
       " 'an_almost_perfect_affair': 'an almost perfect affair',\n",
       " 'an_american_christmas_carol': 'an american christmas carol',\n",
       " \"an_autumn's_tale\": \"an autumn's tale\",\n",
       " 'an_eastern_westerner': 'an eastern westerner',\n",
       " 'an_episode_in_the_life_of_an_iron_picker': 'an episode in the life of an iron picker',\n",
       " 'an_ethics_lesson': 'an ethics lesson',\n",
       " 'an_eye_for_an_eye_(1966_film)': 'an eye for an eye',\n",
       " 'an_eye_for_an_eye_(1981_film)': 'an eye for an eye',\n",
       " 'an_inspector_calls_(1954_film)': 'an inspector calls',\n",
       " 'ana_maria_in_novela_land': 'ana maria in novela land',\n",
       " 'anacondas:_the_hunt_for_the_blood_orchid': 'anacondas: the hunt for the blood orchid',\n",
       " 'anacondas:_trail_of_blood': 'anacondas: trail of blood',\n",
       " 'analyze_that': 'analyze that',\n",
       " 'analyze_this': 'analyze this',\n",
       " 'anand_(2004_film)': 'anand',\n",
       " 'anarkali_(1955_film)': 'anarkali',\n",
       " 'anastasia_(1997_film)': 'anastasia',\n",
       " 'anatomy_(film)': 'anatomy',\n",
       " 'anatomy_of_hell': 'anatomy of hell',\n",
       " 'anatomy_of_a_murder': 'anatomy of a murder',\n",
       " 'anchors_aweigh_(film)': 'anchors aweigh',\n",
       " 'and_along_come_tourists': 'and along come tourists',\n",
       " 'and_god_created_woman_(1956_film)': 'and god created woman',\n",
       " 'and_god_created_woman_(1988_film)': 'and god created woman',\n",
       " 'and_god_said_to_cain': 'and god said to cain',\n",
       " 'and_hope_to_die': 'and hope to die',\n",
       " 'and_love_has_vanished': 'and love has vanished',\n",
       " 'and_so_it_goes_(film)': 'and so it goes',\n",
       " 'and_soon_the_darkness': 'and soon the darkness',\n",
       " 'and_soon_the_darkness_(2010_film)': 'and soon the darkness',\n",
       " 'and_then_came_love': 'and then came love',\n",
       " 'and_then_there_were_none_(1945_film)': 'and then there were none',\n",
       " 'and_then_there_were_none_(1974_film)': 'and then there were none',\n",
       " 'and_when_did_you_last_see_your_father%3f': 'and when did you last see your father?',\n",
       " 'and_you_thought_your_parents_were_weird': 'and you thought your parents were weird',\n",
       " 'and_the_ship_sails_on': 'and the ship sails on',\n",
       " 'andiamo_a_quel_paese': 'andiamo a quel paese',\n",
       " 'andrei_rublev_(film)': 'andrei rublev',\n",
       " 'android_(film)': 'android',\n",
       " 'android_cop': 'android cop',\n",
       " \"andy_hardy's_blonde_trouble\": \"andy hardy's blonde trouble\",\n",
       " \"andy_hardy's_double_life\": \"andy hardy's double life\",\n",
       " \"andy_hardy's_private_secretary\": \"andy hardy's private secretary\",\n",
       " 'andy_hardy_gets_spring_fever': 'andy hardy gets spring fever',\n",
       " 'andy_hardy_meets_debutante': 'andy hardy meets debutante',\n",
       " 'anesthesia_(1929_film)': 'anesthesia',\n",
       " 'angel_(1937_film)': 'angel',\n",
       " 'angel_(1982_greek_film)': 'angel',\n",
       " 'angel_(2007_film)': 'angel',\n",
       " 'angel_eyes_(film)': 'angel eyes',\n",
       " 'angel_face_(1953_film)': 'angel face',\n",
       " 'angel_heart': 'angel heart',\n",
       " 'angel_and_the_badman': 'angel and the badman',\n",
       " 'angel_on_my_shoulder_(film)': 'angel on my shoulder',\n",
       " \"angela's_ashes_(film)\": \"angela's ashes\",\n",
       " 'angelica_(film)': 'angelica',\n",
       " 'angelo_my_love': 'angelo my love',\n",
       " \"angels'_wild_women\": \"angels' wild women\",\n",
       " 'angels_&_demons_(film)': 'angels & demons',\n",
       " 'angels_one_five': 'angels one five',\n",
       " 'angels_over_broadway': 'angels over broadway',\n",
       " 'angels_in_the_infield': 'angels in the infield',\n",
       " 'angels_in_the_outfield_(1951_film)': 'angels in the outfield',\n",
       " 'angels_in_the_outfield_(1994_film)': 'angels in the outfield',\n",
       " 'angels_with_dirty_faces': 'angels with dirty faces',\n",
       " 'anger_management_(film)': 'anger management',\n",
       " 'angie_(1994_film)': 'angie',\n",
       " 'angora_love': 'angora love',\n",
       " 'angry_harvest': 'angry harvest',\n",
       " 'angry_indian_goddesses': 'angry indian goddesses',\n",
       " 'angry_video_game_nerd:_the_movie': 'angry video game nerd: the movie',\n",
       " 'angst_(1928_film)': 'angst',\n",
       " 'angst_(1976_film)': 'angst',\n",
       " 'angst_(1983_film)': 'angst',\n",
       " 'anguish_(1947_film)': 'anguish',\n",
       " 'anguish_(1987_film)': 'anguish',\n",
       " 'anguish_(2015_film)': 'anguish',\n",
       " 'angus,_thongs_and_perfect_snogging': 'angus, thongs and perfect snogging',\n",
       " 'animal_(2005_film)': 'animal',\n",
       " 'animal_(2014_film)': 'animal',\n",
       " 'animal_crackers_(2017_film)': 'animal crackers',\n",
       " 'animal_factory': 'animal factory',\n",
       " 'animal_farm_(1954_film)': 'animal farm',\n",
       " 'animal_kingdom_(film)': 'animal kingdom',\n",
       " 'animals_are_beautiful_people': 'animals are beautiful people',\n",
       " 'animals_united': 'animals united',\n",
       " 'anita_and_me_(film)': 'anita and me',\n",
       " 'anjaan_(2014_film)': 'anjaan',\n",
       " 'anjaana_anjaani': 'anjaana anjaani',\n",
       " 'anjaneya_(film)': 'anjaneya',\n",
       " \"ann_carver's_profession\": \"ann carver's profession\",\n",
       " 'anna_(1951_film)': 'anna',\n",
       " 'anna_(1967_film)': 'anna',\n",
       " 'anna_(1987_film)': 'anna',\n",
       " 'anna_christie_(1930_english-language_film)': 'anna christie',\n",
       " 'anna_christie_(1930_german-language_film)': 'anna christie',\n",
       " 'anna_karenina_(1915_film)': 'anna karenina',\n",
       " 'anna_karenina_(1935_film)': 'anna karenina',\n",
       " 'anna_karenina_(1948_film)': 'anna karenina',\n",
       " 'anna_karenina_(1967_film)': 'anna karenina',\n",
       " 'anna_karenina_(1997_film)': 'anna karenina',\n",
       " 'anna_karenina_(2012_film)': 'anna karenina',\n",
       " 'anna_pavlova_(film)': 'anna pavlova',\n",
       " 'anna_and_the_king': 'anna and the king',\n",
       " 'anna_to_the_infinite_power': 'anna to the infinite power',\n",
       " 'annabelle_(film)': 'annabelle',\n",
       " 'annamayya_(film)': 'annamayya',\n",
       " 'annayum_rasoolum': 'annayum rasoolum',\n",
       " 'anne_of_green_gables_(1919_film)': 'anne of green gables',\n",
       " 'anne_of_green_gables_(1934_film)': 'anne of green gables',\n",
       " 'anne_of_the_indies': 'anne of the indies',\n",
       " 'anne_of_the_thousand_days': 'anne of the thousand days',\n",
       " 'annie_(1982_film)': 'annie',\n",
       " 'annie_(2014_film)': 'annie',\n",
       " 'annie_get_your_gun_(film)': 'annie get your gun',\n",
       " 'annie_hall': 'annie hall',\n",
       " 'annie_laurie_(1916_film)': 'annie laurie',\n",
       " 'annie_laurie_(1927_film)': 'annie laurie',\n",
       " 'annihilation_(film)': 'annihilation',\n",
       " 'anniyan': 'anniyan',\n",
       " 'anomalisa': 'anomalisa',\n",
       " 'anon_(film)': 'anon',\n",
       " 'another_48_hrs.': 'another 48 hrs.',\n",
       " 'another_chance_(film)': 'another chance',\n",
       " 'another_cinderella_story': 'another cinderella story',\n",
       " 'another_country_(film)': 'another country',\n",
       " 'another_dawn_(1943_film)': 'another dawn',\n",
       " 'another_day_(2001_film)': 'another day',\n",
       " 'another_day_in_paradise_(film)': 'another day in paradise',\n",
       " 'another_earth': 'another earth',\n",
       " 'another_face': 'another face',\n",
       " 'another_fine_mess': 'another fine mess',\n",
       " 'another_gay_movie': 'another gay movie',\n",
       " 'another_gay_sequel:_gays_gone_wild!': 'another gay sequel: gays gone wild!',\n",
       " 'another_happy_day': 'another happy day',\n",
       " \"another_man's_poison\": \"another man's poison\",\n",
       " 'another_man,_another_chance': 'another man, another chance',\n",
       " 'another_me_(film)': 'another me',\n",
       " 'another_stakeout': 'another stakeout',\n",
       " 'another_thin_man': 'another thin man',\n",
       " 'another_way_(film)': 'another way',\n",
       " 'another_you': 'another you',\n",
       " 'answers_to_nothing_(film)': 'answers to nothing',\n",
       " 'ant-man_(film)': 'ant-man',\n",
       " 'antarctic_journal': 'antarctic journal',\n",
       " 'antarctica_(1983_film)': 'antarctica',\n",
       " 'anthony_zimmer': 'anthony zimmer',\n",
       " 'antichrist_(film)': 'antichrist',\n",
       " 'antique_(film)': 'antique',\n",
       " 'antitrust_(film)': 'antitrust',\n",
       " 'antiviral_(film)': 'antiviral',\n",
       " 'antonia_(1935_film)': 'antonia',\n",
       " 'antony_and_cleopatra_(1972_film)': 'antony and cleopatra',\n",
       " 'antwone_fisher_(film)': 'antwone fisher',\n",
       " 'antz': 'antz',\n",
       " 'anuvahood': 'anuvahood',\n",
       " 'any_day_now_(2012_film)': 'any day now',\n",
       " 'any_gun_can_play': 'any gun can play',\n",
       " 'any_number_can_win_(film)': 'any number can win',\n",
       " 'any_wednesday': 'any wednesday',\n",
       " 'any_which_way_you_can': 'any which way you can',\n",
       " 'anything_else': 'anything else',\n",
       " 'anything_goes_(1936_film)': 'anything goes',\n",
       " 'anything_goes_(1956_film)': 'anything goes',\n",
       " 'anywhere_but_here_(film)': 'anywhere but here',\n",
       " 'anzio_(film)': 'anzio',\n",
       " 'apache_(film)': 'apache',\n",
       " 'apache_blood': 'apache blood',\n",
       " 'apache_rifles': 'apache rifles',\n",
       " 'apache_territory': 'apache territory',\n",
       " 'apache_trail_(film)': 'apache trail',\n",
       " 'apache_war_smoke': 'apache war smoke',\n",
       " 'apache_warrior': 'apache warrior',\n",
       " 'apaharan': 'apaharan',\n",
       " 'apartment_143': 'apartment 143',\n",
       " 'apartment_zero': 'apartment zero',\n",
       " 'apartment_for_peggy': 'apartment for peggy',\n",
       " 'ape_(1976_film)': 'a*p*e',\n",
       " 'apenas_o_fim': 'apenas o fim',\n",
       " 'apocalypse_now': 'apocalypse now',\n",
       " 'apollo_13_(film)': 'apollo 13',\n",
       " 'apollo_18_(film)': 'apollo 18',\n",
       " 'apoorva_sagodharargal_(1949_film)': 'apoorva sagodharargal',\n",
       " 'applause_(1929_film)': 'applause',\n",
       " 'applesauce_(film)': 'applesauce',\n",
       " 'appointment_with_crime': 'appointment with crime',\n",
       " 'appointment_with_danger': 'appointment with danger',\n",
       " 'appointment_with_death_(film)': 'appointment with death',\n",
       " 'appointment_with_venus_(film)': 'appointment with venus',\n",
       " 'apprentice_to_murder': 'apprentice to murder',\n",
       " 'april_9th_(film)': 'april 9th',\n",
       " \"april_fool's_day_(1986_film)\": \"april fool's day\",\n",
       " 'april_love_(film)': 'april love',\n",
       " 'april_showers_(1923_film)': 'april showers',\n",
       " 'april_showers_(1948_film)': 'april showers',\n",
       " 'april_in_paris_(film)': 'april in paris',\n",
       " 'aquamarine_(film)': 'aquamarine',\n",
       " 'arabesque_(1966_film)': 'arabesque',\n",
       " 'arabian_adventure': 'arabian adventure',\n",
       " 'arabian_nights_(1942_film)': 'arabian nights',\n",
       " 'arabian_nights_(1974_film)': 'arabian nights',\n",
       " 'arachnophobia_(film)': 'arachnophobia',\n",
       " 'arahan': 'arahan',\n",
       " 'ararat_(film)': 'ararat',\n",
       " 'arbitrage_(film)': 'arbitrage',\n",
       " 'arcana_(film)': 'arcana',\n",
       " 'arch_of_triumph_(1948_film)': 'arch of triumph',\n",
       " 'archangel_(2005_film)': 'archangel',\n",
       " \"archie's_final_project\": \"archie's final project\",\n",
       " 'architecture_101': 'architecture 101',\n",
       " 'arctic_blast': 'arctic blast',\n",
       " 'arctic_blue': 'arctic blue',\n",
       " 'arctic_tale': 'arctic tale',\n",
       " 'ardor_(film)': 'ardor',\n",
       " 'are_we_there_yet%3f_(film)': 'are we there yet?',\n",
       " 'are_you_listening%3f_(film)': 'are you listening?',\n",
       " 'arena_(1989_film)': 'arena',\n",
       " ...}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#create dictionary with the movie_id and title\n",
    "movies_dict= dict(zip(train_data_df.movie_id, train_data_df.title))\n",
    "movies_dict\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4319a517de7d4b7ebeb6a4568c5916c0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(HTML(value=''), FloatProgress(value=0.0, max=19492.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Length of training data 19492\n"
     ]
    }
   ],
   "source": [
    "# Create the tagged document needed for Doc2Vec-=\n",
    "tagged_documents=[]\n",
    "train_data_df.progress_apply(lambda row : tagged_documents.append(TaggedDocument(row.movie_sentence,[row.movie_id])),axis=1)\n",
    "\n",
    "#convert the series of tagged docuemnt to a list\n",
    "train_data = list(tagged_documents)\n",
    "\n",
    "print(\"Length of training data {}\".format(len(train_data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed = 400\n",
    "[random.shuffle(data.words) for data in train_data]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "***Configure and train the model ***"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-07 18:38:41,071 : INFO : collecting all words and their counts\n",
      "2020-12-07 18:38:41,073 : INFO : PROGRESS: at example #0, processed 0 words (0/s), 0 word types, 0 tags\n",
      "2020-12-07 18:38:41,120 : INFO : PROGRESS: at example #10000, processed 95094 words (1992256/s), 29478 word types, 10000 tags\n",
      "2020-12-07 18:38:41,176 : INFO : collected 43764 word types and 19491 unique tags from a corpus of 19492 examples and 185880 words\n",
      "2020-12-07 18:38:41,177 : INFO : Loading a fresh vocabulary\n",
      "2020-12-07 18:38:41,302 : INFO : effective_min_count=5 retains 6620 unique words (15% of original 43764, drops 37144)\n",
      "2020-12-07 18:38:41,302 : INFO : effective_min_count=5 leaves 129179 word corpus (69% of original 185880, drops 56701)\n",
      "2020-12-07 18:38:41,324 : INFO : deleting the raw counts dictionary of 43764 items\n",
      "2020-12-07 18:38:41,326 : INFO : sample=0.001 downsamples 32 most-common words\n",
      "2020-12-07 18:38:41,327 : INFO : downsampling leaves estimated 97415 word corpus (75.4% of prior 129179)\n",
      "2020-12-07 18:38:41,350 : INFO : estimated required memory for 6620 words and 100 dimensions: 20300600 bytes\n",
      "2020-12-07 18:38:41,351 : INFO : resetting layer weights\n",
      "2020-12-07 18:38:45,465 : INFO : training model with 8 workers on 6620 vocabulary and 100 features, using sg=1 hs=0 sample=0.001 negative=20 window=5\n",
      "2020-12-07 18:38:47,175 : INFO : EPOCH 1 - PROGRESS: at 48.63% examples, 33100 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:38:47,197 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:38:47,199 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:38:47,220 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:38:47,226 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:38:47,227 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:38:47,324 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:38:47,382 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:38:47,384 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:38:47,385 : INFO : EPOCH - 1 : training on 185880 raw words (116895 effective words) took 1.9s, 61252 effective words/s\n",
      "2020-12-07 18:38:49,043 : INFO : EPOCH 2 - PROGRESS: at 48.39% examples, 34397 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:38:49,067 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:38:49,077 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:38:49,083 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:38:49,088 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:38:49,090 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:38:49,194 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:38:49,246 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:38:49,248 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:38:49,248 : INFO : EPOCH - 2 : training on 185880 raw words (117082 effective words) took 1.8s, 63454 effective words/s\n",
      "2020-12-07 18:38:50,883 : INFO : EPOCH 3 - PROGRESS: at 48.39% examples, 34650 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:38:50,914 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:38:50,923 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:38:50,928 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:38:50,933 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:38:50,935 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:38:51,046 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:38:51,101 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:38:51,108 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:38:51,109 : INFO : EPOCH - 3 : training on 185880 raw words (116967 effective words) took 1.9s, 63205 effective words/s\n",
      "2020-12-07 18:38:52,777 : INFO : EPOCH 4 - PROGRESS: at 48.37% examples, 34026 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:38:52,786 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:38:52,800 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:38:52,807 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:38:52,808 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:38:52,812 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:38:52,910 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:38:52,958 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:38:52,962 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:38:52,962 : INFO : EPOCH - 4 : training on 185880 raw words (116745 effective words) took 1.8s, 63587 effective words/s\n",
      "2020-12-07 18:38:54,529 : INFO : EPOCH 5 - PROGRESS: at 48.39% examples, 36226 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:38:54,549 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:38:54,553 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:38:54,555 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:38:54,566 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:38:54,573 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:38:54,672 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:38:54,722 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:38:54,724 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:38:54,724 : INFO : EPOCH - 5 : training on 185880 raw words (117015 effective words) took 1.8s, 66823 effective words/s\n",
      "2020-12-07 18:38:56,219 : INFO : EPOCH 6 - PROGRESS: at 48.39% examples, 37834 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:38:56,250 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:38:56,266 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:38:56,272 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:38:56,275 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:38:56,279 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:38:56,374 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:38:56,415 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:38:56,421 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:38:56,422 : INFO : EPOCH - 6 : training on 185880 raw words (116822 effective words) took 1.7s, 69235 effective words/s\n",
      "2020-12-07 18:38:57,838 : INFO : EPOCH 7 - PROGRESS: at 48.39% examples, 39963 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:38:57,851 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:38:57,863 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:38:57,869 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:38:57,871 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:38:57,873 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:38:57,967 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:38:58,017 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:38:58,018 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:38:58,019 : INFO : EPOCH - 7 : training on 185880 raw words (116851 effective words) took 1.6s, 73629 effective words/s\n",
      "2020-12-07 18:38:59,542 : INFO : EPOCH 8 - PROGRESS: at 48.39% examples, 37023 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:38:59,557 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:38:59,568 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:38:59,573 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:38:59,579 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:38:59,583 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:38:59,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:38:59,745 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:38:59,749 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:38:59,749 : INFO : EPOCH - 8 : training on 185880 raw words (116900 effective words) took 1.7s, 67872 effective words/s\n",
      "2020-12-07 18:39:01,641 : INFO : EPOCH 9 - PROGRESS: at 48.39% examples, 30001 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:01,662 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:01,665 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:01,670 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:01,683 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:01,685 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:01,873 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:01,958 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:01,964 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:01,966 : INFO : EPOCH - 9 : training on 185880 raw words (116913 effective words) took 2.2s, 53120 effective words/s\n",
      "2020-12-07 18:39:02,993 : INFO : EPOCH 10 - PROGRESS: at 26.65% examples, 31139 words/s, in_qsize 14, out_qsize 0\n",
      "2020-12-07 18:39:03,965 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:03,979 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:03,992 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:03,997 : INFO : EPOCH 10 - PROGRESS: at 80.48% examples, 46845 words/s, in_qsize 3, out_qsize 3\n",
      "2020-12-07 18:39:03,998 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:03,999 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:04,122 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:04,187 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:04,189 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:04,189 : INFO : EPOCH - 10 : training on 185880 raw words (116699 effective words) took 2.2s, 53117 effective words/s\n",
      "2020-12-07 18:39:05,217 : INFO : EPOCH 11 - PROGRESS: at 5.22% examples, 6138 words/s, in_qsize 15, out_qsize 0\n",
      "2020-12-07 18:39:06,232 : INFO : EPOCH 11 - PROGRESS: at 48.39% examples, 27772 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:06,261 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:06,278 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:06,282 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:06,284 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:06,286 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:06,419 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:06,485 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:06,487 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:06,488 : INFO : EPOCH - 11 : training on 185880 raw words (117007 effective words) took 2.3s, 51244 effective words/s\n",
      "2020-12-07 18:39:07,505 : INFO : EPOCH 12 - PROGRESS: at 10.58% examples, 12291 words/s, in_qsize 15, out_qsize 0\n",
      "2020-12-07 18:39:08,469 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:08,481 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:08,496 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:08,500 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:08,506 : INFO : EPOCH 12 - PROGRESS: at 86.08% examples, 50055 words/s, in_qsize 3, out_qsize 1\n",
      "2020-12-07 18:39:08,507 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:08,636 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:08,699 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:08,705 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:08,707 : INFO : EPOCH - 12 : training on 185880 raw words (116919 effective words) took 2.2s, 52955 effective words/s\n",
      "2020-12-07 18:39:10,425 : INFO : EPOCH 13 - PROGRESS: at 48.39% examples, 33152 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:10,467 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:10,471 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:10,475 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:10,480 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:10,484 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:10,593 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:10,645 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:10,655 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:10,656 : INFO : EPOCH - 13 : training on 185880 raw words (116958 effective words) took 1.9s, 60665 effective words/s\n",
      "2020-12-07 18:39:11,684 : INFO : EPOCH 14 - PROGRESS: at 32.10% examples, 36918 words/s, in_qsize 13, out_qsize 0\n",
      "2020-12-07 18:39:12,617 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:12,636 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:12,642 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:12,644 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:12,650 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:12,775 : INFO : EPOCH 14 - PROGRESS: at 89.36% examples, 49447 words/s, in_qsize 2, out_qsize 1\n",
      "2020-12-07 18:39:12,777 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:12,834 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:12,839 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:12,840 : INFO : EPOCH - 14 : training on 185880 raw words (116655 effective words) took 2.2s, 53805 effective words/s\n",
      "2020-12-07 18:39:14,677 : INFO : EPOCH 15 - PROGRESS: at 48.33% examples, 30801 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:14,691 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:14,711 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:14,719 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:14,723 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:14,724 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:14,840 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:14,897 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:14,905 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:14,907 : INFO : EPOCH - 15 : training on 185880 raw words (116864 effective words) took 2.1s, 56816 effective words/s\n",
      "2020-12-07 18:39:16,624 : INFO : EPOCH 16 - PROGRESS: at 48.39% examples, 32976 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:16,645 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:16,657 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:16,666 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:16,668 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:16,671 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:16,779 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:16,831 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:16,834 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:16,835 : INFO : EPOCH - 16 : training on 185880 raw words (116765 effective words) took 1.9s, 60877 effective words/s\n",
      "2020-12-07 18:39:18,660 : INFO : EPOCH 17 - PROGRESS: at 48.33% examples, 31088 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:18,689 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:18,695 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:18,702 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:18,709 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:18,713 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:18,837 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:18,887 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:18,891 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:18,892 : INFO : EPOCH - 17 : training on 185880 raw words (116997 effective words) took 2.0s, 57225 effective words/s\n",
      "2020-12-07 18:39:20,737 : INFO : EPOCH 18 - PROGRESS: at 48.39% examples, 30738 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:20,762 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:20,791 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:20,793 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:20,797 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:20,802 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:20,926 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:20,983 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:20,984 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:20,985 : INFO : EPOCH - 18 : training on 185880 raw words (117067 effective words) took 2.1s, 56248 effective words/s\n",
      "2020-12-07 18:39:22,848 : INFO : EPOCH 19 - PROGRESS: at 48.39% examples, 30341 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:22,872 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:22,876 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:22,878 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:22,883 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:22,888 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:23,010 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:23,077 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:23,079 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:23,079 : INFO : EPOCH - 19 : training on 185880 raw words (116894 effective words) took 2.1s, 56105 effective words/s\n",
      "2020-12-07 18:39:24,946 : INFO : EPOCH 20 - PROGRESS: at 48.39% examples, 30227 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:24,971 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:24,992 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:24,995 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:24,997 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:24,998 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:25,127 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:25,181 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:25,188 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:25,188 : INFO : EPOCH - 20 : training on 185880 raw words (116790 effective words) took 2.1s, 55683 effective words/s\n",
      "2020-12-07 18:39:27,018 : INFO : EPOCH 21 - PROGRESS: at 48.39% examples, 30910 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:27,049 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:27,052 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:27,055 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:27,057 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:27,062 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:27,183 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:27,235 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:27,239 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:27,240 : INFO : EPOCH - 21 : training on 185880 raw words (116824 effective words) took 2.0s, 57245 effective words/s\n",
      "2020-12-07 18:39:29,029 : INFO : EPOCH 22 - PROGRESS: at 48.39% examples, 31355 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:29,077 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:29,083 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:29,086 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:29,094 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:29,101 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:29,215 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:29,272 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:29,273 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:29,273 : INFO : EPOCH - 22 : training on 185880 raw words (116853 effective words) took 2.0s, 57751 effective words/s\n",
      "2020-12-07 18:39:30,965 : INFO : EPOCH 23 - PROGRESS: at 48.39% examples, 33539 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:30,990 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:31,003 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:31,009 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:31,014 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:31,015 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:31,128 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:31,186 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:31,191 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:31,192 : INFO : EPOCH - 23 : training on 185880 raw words (116828 effective words) took 1.9s, 61428 effective words/s\n",
      "2020-12-07 18:39:32,904 : INFO : EPOCH 24 - PROGRESS: at 48.39% examples, 32907 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:32,920 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:32,936 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:32,944 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:32,946 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:32,950 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:33,065 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:33,138 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:33,140 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:33,141 : INFO : EPOCH - 24 : training on 185880 raw words (116709 effective words) took 1.9s, 60134 effective words/s\n",
      "2020-12-07 18:39:34,158 : INFO : EPOCH 25 - PROGRESS: at 5.22% examples, 6176 words/s, in_qsize 15, out_qsize 0\n",
      "2020-12-07 18:39:35,281 : INFO : EPOCH 25 - PROGRESS: at 48.39% examples, 26447 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:35,325 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:35,338 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:35,350 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:35,358 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:35,360 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:35,503 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:35,570 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:35,580 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:35,581 : INFO : EPOCH - 25 : training on 185880 raw words (116957 effective words) took 2.4s, 48140 effective words/s\n",
      "2020-12-07 18:39:36,692 : INFO : EPOCH 26 - PROGRESS: at 5.22% examples, 5665 words/s, in_qsize 15, out_qsize 0\n",
      "2020-12-07 18:39:37,802 : INFO : EPOCH 26 - PROGRESS: at 48.39% examples, 25538 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:37,829 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:37,848 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:37,851 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:37,855 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:37,856 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:38,007 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:38,066 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:38,070 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:38,071 : INFO : EPOCH - 26 : training on 185880 raw words (116856 effective words) took 2.5s, 47255 effective words/s\n",
      "2020-12-07 18:39:39,938 : INFO : EPOCH 27 - PROGRESS: at 48.39% examples, 30388 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:39,962 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:39,980 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:39,985 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:39,987 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:39,989 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:40,116 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:40,175 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:40,178 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:40,178 : INFO : EPOCH - 27 : training on 185880 raw words (116803 effective words) took 2.1s, 55853 effective words/s\n",
      "2020-12-07 18:39:41,968 : INFO : EPOCH 28 - PROGRESS: at 48.39% examples, 31699 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:41,990 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:42,004 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:42,013 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:42,017 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:42,019 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:42,143 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:42,202 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:42,208 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:42,209 : INFO : EPOCH - 28 : training on 185880 raw words (117117 effective words) took 2.0s, 57999 effective words/s\n",
      "2020-12-07 18:39:43,233 : INFO : EPOCH 29 - PROGRESS: at 10.54% examples, 12611 words/s, in_qsize 15, out_qsize 0\n",
      "2020-12-07 18:39:44,204 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:44,208 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:44,218 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:44,222 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:44,225 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:44,346 : INFO : EPOCH 29 - PROGRESS: at 89.36% examples, 49290 words/s, in_qsize 2, out_qsize 1\n",
      "2020-12-07 18:39:44,348 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:44,416 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:44,418 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:44,419 : INFO : EPOCH - 29 : training on 185880 raw words (117101 effective words) took 2.2s, 53471 effective words/s\n",
      "2020-12-07 18:39:46,278 : INFO : EPOCH 30 - PROGRESS: at 48.39% examples, 30623 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:46,306 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:46,312 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:46,325 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:46,332 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:46,339 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:46,475 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:46,538 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:46,546 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:46,547 : INFO : EPOCH - 30 : training on 185880 raw words (116889 effective words) took 2.1s, 55430 effective words/s\n",
      "2020-12-07 18:39:47,564 : INFO : EPOCH 31 - PROGRESS: at 43.02% examples, 49668 words/s, in_qsize 11, out_qsize 0\n",
      "2020-12-07 18:39:48,576 : INFO : EPOCH 31 - PROGRESS: at 53.99% examples, 31056 words/s, in_qsize 9, out_qsize 0\n",
      "2020-12-07 18:39:48,582 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:48,585 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:48,587 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:48,598 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:48,600 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:48,725 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:48,780 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:48,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:48,787 : INFO : EPOCH - 31 : training on 185880 raw words (117047 effective words) took 2.2s, 52493 effective words/s\n",
      "2020-12-07 18:39:50,666 : INFO : EPOCH 32 - PROGRESS: at 48.39% examples, 30063 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:50,683 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:50,692 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:50,709 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:50,713 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:50,716 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:50,841 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:50,897 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:50,906 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:50,906 : INFO : EPOCH - 32 : training on 185880 raw words (116743 effective words) took 2.1s, 55339 effective words/s\n",
      "2020-12-07 18:39:52,817 : INFO : EPOCH 33 - PROGRESS: at 48.39% examples, 29658 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:52,835 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:52,850 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:52,855 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:52,867 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:52,871 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:52,997 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:53,053 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:53,058 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:53,059 : INFO : EPOCH - 33 : training on 185880 raw words (116945 effective words) took 2.1s, 54676 effective words/s\n",
      "2020-12-07 18:39:54,079 : INFO : EPOCH 34 - PROGRESS: at 43.02% examples, 49920 words/s, in_qsize 11, out_qsize 0\n",
      "2020-12-07 18:39:55,113 : INFO : EPOCH 34 - PROGRESS: at 48.39% examples, 27625 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:55,157 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:55,170 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:55,177 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:55,185 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:55,188 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:55,334 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:55,392 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:55,397 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:55,398 : INFO : EPOCH - 34 : training on 185880 raw words (116813 effective words) took 2.3s, 50319 effective words/s\n",
      "2020-12-07 18:39:56,479 : INFO : EPOCH 35 - PROGRESS: at 5.22% examples, 5770 words/s, in_qsize 16, out_qsize 0\n",
      "2020-12-07 18:39:57,543 : INFO : EPOCH 35 - PROGRESS: at 48.39% examples, 26335 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:57,570 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:39:57,587 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:39:57,590 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:39:57,597 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:39:57,604 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:39:57,745 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:39:57,809 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:39:57,810 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:39:57,810 : INFO : EPOCH - 35 : training on 185880 raw words (116865 effective words) took 2.4s, 48627 effective words/s\n",
      "2020-12-07 18:39:58,855 : INFO : EPOCH 36 - PROGRESS: at 5.22% examples, 5971 words/s, in_qsize 15, out_qsize 0\n",
      "2020-12-07 18:39:59,974 : INFO : EPOCH 36 - PROGRESS: at 48.39% examples, 26253 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:39:59,999 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:00,016 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:00,025 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:00,030 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:00,035 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:00,187 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:00,261 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:00,266 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:00,267 : INFO : EPOCH - 36 : training on 185880 raw words (116833 effective words) took 2.4s, 47923 effective words/s\n",
      "2020-12-07 18:40:01,407 : INFO : EPOCH 37 - PROGRESS: at 5.22% examples, 5529 words/s, in_qsize 15, out_qsize 0\n",
      "2020-12-07 18:40:02,523 : INFO : EPOCH 37 - PROGRESS: at 48.39% examples, 25198 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:40:02,552 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:02,566 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:02,572 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:02,584 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:02,587 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:02,722 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:02,803 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:02,811 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:02,812 : INFO : EPOCH - 37 : training on 185880 raw words (116988 effective words) took 2.5s, 46330 effective words/s\n",
      "2020-12-07 18:40:04,684 : INFO : EPOCH 38 - PROGRESS: at 48.39% examples, 30263 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:40:04,702 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:04,716 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:04,718 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:04,728 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:04,731 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:04,851 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:04,908 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:04,912 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:04,912 : INFO : EPOCH - 38 : training on 185880 raw words (116862 effective words) took 2.1s, 55941 effective words/s\n",
      "2020-12-07 18:40:06,772 : INFO : EPOCH 39 - PROGRESS: at 48.39% examples, 30448 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:40:06,801 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:06,810 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:06,821 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:06,823 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:06,826 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:06,943 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:07,008 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:07,010 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:07,011 : INFO : EPOCH - 39 : training on 185880 raw words (116964 effective words) took 2.1s, 56012 effective words/s\n",
      "2020-12-07 18:40:08,025 : INFO : EPOCH 40 - PROGRESS: at 10.54% examples, 12633 words/s, in_qsize 15, out_qsize 0\n",
      "2020-12-07 18:40:09,171 : INFO : EPOCH 40 - PROGRESS: at 48.39% examples, 26199 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:40:09,209 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:09,229 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:09,240 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:09,248 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:09,253 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:09,501 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:09,596 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:09,607 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:09,608 : INFO : EPOCH - 40 : training on 185880 raw words (116955 effective words) took 2.6s, 45207 effective words/s\n",
      "2020-12-07 18:40:10,948 : INFO : EPOCH 41 - PROGRESS: at 5.22% examples, 4725 words/s, in_qsize 15, out_qsize 0\n",
      "2020-12-07 18:40:12,261 : INFO : EPOCH 41 - PROGRESS: at 48.39% examples, 21373 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:40:12,312 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:12,318 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:12,323 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:12,330 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:12,337 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:12,475 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:12,532 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:12,537 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:12,537 : INFO : EPOCH - 41 : training on 185880 raw words (116764 effective words) took 2.9s, 40148 effective words/s\n",
      "2020-12-07 18:40:13,553 : INFO : EPOCH 42 - PROGRESS: at 43.02% examples, 49747 words/s, in_qsize 11, out_qsize 0\n",
      "2020-12-07 18:40:14,487 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:14,500 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:14,503 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:14,513 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:14,515 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:14,645 : INFO : EPOCH 42 - PROGRESS: at 89.36% examples, 49706 words/s, in_qsize 2, out_qsize 1\n",
      "2020-12-07 18:40:14,647 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:14,702 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:14,706 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:14,707 : INFO : EPOCH - 42 : training on 185880 raw words (116928 effective words) took 2.2s, 54149 effective words/s\n",
      "2020-12-07 18:40:16,643 : INFO : EPOCH 43 - PROGRESS: at 48.33% examples, 29253 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:40:16,668 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:16,671 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:16,679 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:16,684 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:16,689 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:16,813 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:16,878 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:16,892 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:16,893 : INFO : EPOCH - 43 : training on 185880 raw words (116906 effective words) took 2.2s, 53771 effective words/s\n",
      "2020-12-07 18:40:17,904 : INFO : EPOCH 44 - PROGRESS: at 26.74% examples, 31231 words/s, in_qsize 14, out_qsize 0\n",
      "2020-12-07 18:40:18,866 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:18,888 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:18,889 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:18,891 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:18,895 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:19,020 : INFO : EPOCH 44 - PROGRESS: at 89.36% examples, 49191 words/s, in_qsize 2, out_qsize 1\n",
      "2020-12-07 18:40:19,022 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:19,079 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:19,081 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:19,082 : INFO : EPOCH - 44 : training on 185880 raw words (116844 effective words) took 2.2s, 53640 effective words/s\n",
      "2020-12-07 18:40:20,098 : INFO : EPOCH 45 - PROGRESS: at 43.02% examples, 49959 words/s, in_qsize 11, out_qsize 0\n",
      "2020-12-07 18:40:21,104 : INFO : EPOCH 45 - PROGRESS: at 48.39% examples, 28066 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:40:21,122 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:21,131 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:21,141 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:21,147 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:21,150 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:21,282 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:21,345 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:21,347 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:21,349 : INFO : EPOCH - 45 : training on 185880 raw words (117029 effective words) took 2.3s, 52004 effective words/s\n",
      "2020-12-07 18:40:22,400 : INFO : EPOCH 46 - PROGRESS: at 5.22% examples, 5984 words/s, in_qsize 15, out_qsize 0\n",
      "2020-12-07 18:40:23,518 : INFO : EPOCH 46 - PROGRESS: at 48.39% examples, 26084 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:40:23,542 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:23,561 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:23,566 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:23,568 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:23,572 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:23,713 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:23,781 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:23,786 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:23,787 : INFO : EPOCH - 46 : training on 185880 raw words (116811 effective words) took 2.4s, 48168 effective words/s\n",
      "2020-12-07 18:40:24,981 : INFO : EPOCH 47 - PROGRESS: at 5.22% examples, 5259 words/s, in_qsize 15, out_qsize 0\n",
      "2020-12-07 18:40:26,112 : INFO : EPOCH 47 - PROGRESS: at 48.39% examples, 24357 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:40:26,149 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:26,159 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:26,173 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:26,177 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:26,184 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:26,325 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:26,388 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:26,390 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:26,392 : INFO : EPOCH - 47 : training on 185880 raw words (116874 effective words) took 2.6s, 45170 effective words/s\n",
      "2020-12-07 18:40:28,297 : INFO : EPOCH 48 - PROGRESS: at 48.39% examples, 29803 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:40:28,317 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:28,333 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:28,338 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:28,343 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:28,347 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:28,487 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:28,559 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:28,560 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:28,561 : INFO : EPOCH - 48 : training on 185880 raw words (116839 effective words) took 2.1s, 54345 effective words/s\n",
      "2020-12-07 18:40:29,573 : INFO : EPOCH 49 - PROGRESS: at 43.02% examples, 49860 words/s, in_qsize 11, out_qsize 0\n",
      "2020-12-07 18:40:30,506 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:30,510 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:30,515 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:30,523 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:30,531 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:30,679 : INFO : EPOCH 49 - PROGRESS: at 89.36% examples, 49414 words/s, in_qsize 2, out_qsize 1\n",
      "2020-12-07 18:40:30,680 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:30,735 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:30,738 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:30,739 : INFO : EPOCH - 49 : training on 185880 raw words (116786 effective words) took 2.2s, 53897 effective words/s\n",
      "2020-12-07 18:40:32,626 : INFO : EPOCH 50 - PROGRESS: at 48.39% examples, 30054 words/s, in_qsize 10, out_qsize 0\n",
      "2020-12-07 18:40:32,648 : INFO : worker thread finished; awaiting finish of 7 more threads\n",
      "2020-12-07 18:40:32,662 : INFO : worker thread finished; awaiting finish of 6 more threads\n",
      "2020-12-07 18:40:32,666 : INFO : worker thread finished; awaiting finish of 5 more threads\n",
      "2020-12-07 18:40:32,675 : INFO : worker thread finished; awaiting finish of 4 more threads\n",
      "2020-12-07 18:40:32,677 : INFO : worker thread finished; awaiting finish of 3 more threads\n",
      "2020-12-07 18:40:32,829 : INFO : worker thread finished; awaiting finish of 2 more threads\n",
      "2020-12-07 18:40:32,902 : INFO : worker thread finished; awaiting finish of 1 more threads\n",
      "2020-12-07 18:40:32,907 : INFO : worker thread finished; awaiting finish of 0 more threads\n",
      "2020-12-07 18:40:32,908 : INFO : EPOCH - 50 : training on 185880 raw words (116832 effective words) took 2.2s, 54267 effective words/s\n",
      "2020-12-07 18:40:32,909 : INFO : training on a 9294000 raw words (5844370 effective words) took 107.4s, 54395 effective words/s\n"
     ]
    }
   ],
   "source": [
    "cores = multiprocessing.cpu_count()\n",
    "model = gensim.models.doc2vec.Doc2Vec(dm= 0, dbow_words=1, vector_size= 100, \n",
    "                                      hs=0 ,min_count=5,\n",
    "                                      workers=cores, epochs= 50,\n",
    "                                      ns_exponent=(-3), negative= 20, window_size= 1000000)\n",
    "model.build_vocab(train_data)\n",
    "model.train(train_data, total_examples=len(train_data) , epochs=model.epochs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "2020-12-07 21:13:40,270 : INFO : saving Doc2Vec object under ../expirements/exp3/model, separately None\n",
      "2020-12-07 21:13:40,659 : INFO : saved ../expirements/exp3/model\n"
     ]
    }
   ],
   "source": [
    "model.save('../expirements/exp3/model')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "19492"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#get the words vectors represenation\n",
    "words_vectors = model.wv\n",
    "#get the docs vectors represenation\n",
    "docs_vectors = model.docvecs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5224008\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "\"word 'douglas_hickox' not in vocabulary\"",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-29-99fb62b3abd0>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"adam_sandler\"\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;34m\"jennifer_aniston\"\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msimilarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'douglas_hickox'\u001b[0m\u001b[1;33m,\u001b[0m\u001b[1;34m'cy_endfield'\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;32md:\\programs\\python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36msimilarity\u001b[1;34m(self, w1, w2)\u001b[0m\n\u001b[0;32m    972\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    973\u001b[0m         \"\"\"\n\u001b[1;32m--> 974\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mdot\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw1\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmatutils\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0munitvec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mw2\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    975\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    976\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mn_similarity\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mws2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36m__getitem__\u001b[1;34m(self, entities)\u001b[0m\n\u001b[0;32m    351\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mstring_types\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    352\u001b[0m             \u001b[1;31m# allow calls like trained_model['office'], as a shorthand for trained_model[['office']]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 353\u001b[1;33m             \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentities\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    354\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    355\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mvstack\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mentity\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mentity\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mentities\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mget_vector\u001b[1;34m(self, word)\u001b[0m\n\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 471\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mword_vec\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    472\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    473\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mwords_closer_than\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw1\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mw2\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mword_vec\u001b[1;34m(self, word, use_norm)\u001b[0m\n\u001b[0;32m    466\u001b[0m             \u001b[1;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    467\u001b[0m         \u001b[1;32melse\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 468\u001b[1;33m             \u001b[1;32mraise\u001b[0m \u001b[0mKeyError\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m\"word '%s' not in vocabulary\"\u001b[0m \u001b[1;33m%\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    469\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    470\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0mget_vector\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mword\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mKeyError\u001b[0m: \"word 'douglas_hickox' not in vocabulary\""
     ]
    }
   ],
   "source": [
    "print(words_vectors.similarity(\"adam_sandler\", \"jennifer_aniston\"))\n",
    "print(words_vectors.similarity('douglas_hickox','cy_endfield'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdin",
     "output_type": "stream",
     "text": [
      "Enter your value:  toy story\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "toy story\n"
     ]
    }
   ],
   "source": [
    "val = input(\"Enter your value: \") \n",
    "print(val) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "'<' not supported between instances of 'str' and 'int'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-38-49ce700b03f8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0msim\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdocs_vectors\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmost_similar\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mpositive\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;34m'toy_story'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mtopn\u001b[0m\u001b[1;33m=\u001b[0m \u001b[1;36m5\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      2\u001b[0m \u001b[0meval_fn\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprint_sim_list\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msim\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32md:\\programs\\python39\\lib\\site-packages\\gensim\\models\\keyedvectors.py\u001b[0m in \u001b[0;36mmost_similar\u001b[1;34m(self, positive, negative, topn, clip_start, clip_end, indexer)\u001b[0m\n\u001b[0;32m   1726\u001b[0m             \u001b[1;32mif\u001b[0m \u001b[0misinstance\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mndarray\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1727\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mdoc\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m-> 1728\u001b[1;33m             \u001b[1;32melif\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctags\u001b[0m \u001b[1;32mor\u001b[0m \u001b[0mdoc\u001b[0m \u001b[1;33m<\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mcount\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m   1729\u001b[0m                 \u001b[0mmean\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mweight\u001b[0m \u001b[1;33m*\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvectors_docs_norm\u001b[0m\u001b[1;33m[\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_int_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m   1730\u001b[0m                 \u001b[0mall_docs\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0madd\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0m_int_index\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mdoc\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mdoctags\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_rawint\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mTypeError\u001b[0m: '<' not supported between instances of 'str' and 'int'"
     ]
    }
   ],
   "source": [
    "sim = docs_vectors.most_similar(positive= 'toy_story', topn= 5)\n",
    "eval_fn.print_sim_list(sim)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.2068108143284917"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sum(docs_vectors['man_of_the_year_(2006_film)']- docs_vectors['mean_girls'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_sim_list(sim_list):\n",
    "    print(tabulate(sim_list, headers= ['Movie ID', 'Similarity']))\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m = 'scrooge_(1951_film)'\n",
    "sim_list = docs_vectors.most_similar(positive= m, topn= 5)\n",
    "results = calculate_BLEU_score(m, sim_list, train_data_df)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_data_df[train_data_df['movie_id'].str.lower() == 'harry_potter_and_the_prisoner_of_azkaban_(film)'.lower()].movie_sentence\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = helper_fn.find_movie_sent_by_id('oliver_twist_(1948_film)',train_data_df)\n",
    "m2 = helper_fn.find_movie_sent_by_id('harry_potter_and_the_chamber_of_secrets_(film)', train_data_df)\n",
    "smoothie = SmoothingFunction().method4\n",
    "sentence_bleu([m1], m2, smoothing_function=smoothie, weights=(1,0,0,0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jaccard(set(m1), set(m2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from jiwer import wer\n",
    "import jiwer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "error = wer(m1, m2)\n",
    "print(error)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "jiwer.wer(m1,m2)\n",
    "jiwer.mer(m1,m2)\n",
    "jiwer.wil(m1,m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import Counter\n",
    "import math\n",
    "\n",
    "m1 = helper_fn.find_movie_sent_by_id('harry_potter_and_the_prisoner_of_azkaban_(film)',train_data_df)\n",
    "m2 = helper_fn.find_movie_sent_by_id('harry_potter_and_the_chamber_of_secrets_(film)', train_data_df)\n",
    "\n",
    "counterA = Counter(m1)\n",
    "counterB = Counter(m2)\n",
    "\n",
    "\n",
    "def counter_cosine_similarity(c1, c2):\n",
    "    terms = set(c1).union(c2)\n",
    "    dotprod = sum(c1.get(k, 0) * c2.get(k, 0) for k in terms)\n",
    "    magA = math.sqrt(sum(c1.get(k, 0)**2 for k in terms))\n",
    "    magB = math.sqrt(sum(c2.get(k, 0)**2 for k in terms))\n",
    "    return dotprod / (magA * magB)\n",
    "\n",
    "print(counter_cosine_similarity(counterA, counterB))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = helper_fn.find_movie_sent_by_id('harry_potter_and_the_prisoner_of_azkaban_(film)',train_data_df)\n",
    "x2 = helper_fn.find_movie_sent_by_id('harry_potter_and_the_chamber_of_secrets_(film)',train_data_df)\n",
    "\n",
    "def jaccard(a, b):\n",
    "    c = a.intersection(b)\n",
    "    return float(len(c)) / (len(a) + len(b) - len(c))\n",
    "\n",
    "jaccard(set(x1), set(x2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_fn.find_common_attrs(train_data_df, 'harry_potter_and_the_prisoner_of_azkaban_(film)','harry_potter_and_the_chamber_of_secrets_(film)')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sent = helper_fn.find_movie_sent_by_id('the_hunchback_of_notre_dame_(1911_film)',train_data_df)\n",
    "print(sent[0])\n",
    "s= ['tobey_aguire','kirsten_dunst']\n",
    "\n",
    "infered_movie = model.infer_vector(s)\n",
    "print(infered_movie)\n",
    "similar_to_infered = docs_vectors.most_similar([infered_movie], topn=20)\n",
    "\n",
    "print(*similar_to_infered, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def start_param_search(train_data_df):\n",
    "    #prepare train\n",
    "    tagged_documents=[]\n",
    "    train_data_df.progress_apply(lambda row : tagged_documents.append(TaggedDocument(row.movie_sentence,[row.movie_id])),axis=1)\n",
    "    train_data = list(tagged_documents)\n",
    "    random.seed = 400\n",
    "    [random.shuffle(data.words) for data in train_data]\n",
    "    random.shuffle(train_data)\n",
    "    \n",
    "    #prepare test data\n",
    "    test_titles = []\n",
    "    test_titles= ['shrek','star trek','toy story','blade','pitch perfect','harry potter','godzilla','paddington','x-men']\n",
    "    test_data= eval_fn.build_movie_sequel_test_data(train_data_df, test_titles)\n",
    "    \n",
    "    #prepare parameters list\n",
    "    top=5\n",
    "    dm = [0]\n",
    "    vector_size = [100,300]\n",
    "    window = [100000]\n",
    "    hs = [0]\n",
    "    min_count=[1,5]\n",
    "    epochs=[50]\n",
    "    ns_exponent=[3,-3]\n",
    "    dbow_words=[1]\n",
    "    negative=[5,20]\n",
    "    param_list = [{'dm': item[0],'vector_size': item[1],'window': item[2],'hs': item[3],'epochs':item[4], 'min_count':item[5],'ns_exponent':item[6], 'dbow_words':item[7],'negative':item[8] }\n",
    "                  for item in\n",
    "                         list(itertools.product(*[dm, vector_size, window, hs, epochs, min_count, ns_exponent, dbow_words, negative]))\n",
    "                ]\n",
    "    print(len(param_list))\n",
    "    #print(len(param_list))\n",
    "    models_scores = eval_fn.model_parameters_search (train_data, test_data, param_list, train_data_df, top)\n",
    "\n",
    "    score_doc2vec = pd.DataFrame(models_scores)\n",
    "    score_doc2vec = score_doc2vec.sort_values(by=['rank_score'],\n",
    "                                             ascending=False)\n",
    "    return score_doc2vec\n",
    "    #call the search function\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "imp.reload(eval_fn)\n",
    "eval_score_df = start_param_search(train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "eval_score_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_eval_score_df = eval_score_df.copy()\n",
    "save_eval_score_df.drop(columns= ['model'],inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "save_eval_score_df.to_excel('models/grid_search_exp/GS_2.xlsx', index=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def build_movie_sequel_test_data(train_df, titles_list):\n",
    "    test_data_list = []\n",
    "    for title in titles_list:\n",
    "        \n",
    "        movies_list = find_movie_sequel(train_df, title)\n",
    "        if movies_list is not None:\n",
    "            test_data_list.append(movies_list)\n",
    "    return test_data_list   \n",
    "\n",
    "def find_movie_sequel(df, movie_name):\n",
    "    movie_name_regex= '^' + movie_name + \"(.*)$\"\n",
    "    #print(movie_name)\n",
    "    #print(df)\n",
    "    matched_movies = df[df['title'].str.match(movie_name_regex.lower())]\n",
    "    return matched_movies.movie_id.values.tolist() if len(matched_movies) >0 else None\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "ranks, second_ranks = calculate_self_similarity(model, train_data[0:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "counter = collections.Counter(second_ranks)\n",
    "print(counter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def construct_titles_list(titles):\n",
    "    return [title for title in titles]\n",
    "\n",
    "sequel_titles =[train_data_df[train_data_df.title.str.match('^shrek(.*)$')]['movie_id'],\n",
    "train_data_df[train_data_df.title.str.match('^star trek(.*)$')]['movie_id'],\n",
    "train_data_df[train_data_df.title.str.match('^toy story(.*)$')]['movie_id'],\n",
    "train_data_df[train_data_df.title.str.match('^the avengers(.*)$')]['movie_id'],\n",
    "train_data_df[train_data_df.title.str.match('^iron man(.*)$')]['movie_id'],\n",
    "train_data_df[train_data_df.title.str.match('^cars(.*)$')]['movie_id'],\n",
    "train_data_df[train_data_df.title.str.match('^blade(.*)$')]['movie_id'],\n",
    "train_data_df[train_data_df.title.str.match('^pitch perfect(.*)$')]['movie_id'],\n",
    "train_data_df[train_data_df.title.str.match('^harry potter(.*)$')]['movie_id'],\n",
    "train_data_df[train_data_df.title.str.match('^godzilla(.*)$')]['movie_id'],\n",
    "train_data_df[train_data_df.title.str.match('^paddington(.*)$')]['movie_id'],\n",
    "train_data_df[train_data_df.title.str.match('^spider(.*)$')]['movie_id'],\n",
    "train_data_df[train_data_df.title.str.match('^x-men(.*)$')]['movie_id']]\n",
    "def construct_titles_list(titles):\n",
    "    return [title for title in titles]\n",
    "for sequel in sequel_titles:\n",
    "    test_titles= test_titles + construct_titles_list(sequel)\n",
    "\n",
    "test_titles = test_titles + doc_tags[5000:5050]\n",
    "tsne_plot_doc2vec(model, test_titles, movies_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data_df['genre'].unique"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "m1 = helper_fn.find_movie_id_by_title('truck turner',train_data_df)\n",
    "m2 = helper_fn.find_movie_id_by_title('stealth fighter',train_data_df)\n",
    "#print(m1,m2)\n",
    "print(m1[0])\n",
    "print(m2[0])\n",
    "x = model[m1[0]] - model[m2[0]]\n",
    "print(sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m1 = helper_fn.find_movie_id_by_title('the girl in the red velvet swing',train_data_df)\n",
    "m2 = helper_fn.find_movie_id_by_title('the thirteenth guest',train_data_df)\n",
    "#print(m1,m2)\n",
    "print(m1[0])\n",
    "print(m2[0])\n",
    "x = model[m1[0]] - model[m2[0]]\n",
    "print(sum(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_tags = list(model.docvecs.doctags.keys())\n",
    "sample_tags= doc_tags[0:1000]\n",
    "sample_docs = model[sample_tags]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tsne_plot_doc2vec(model, samples,movies_dict):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    \n",
    "\n",
    "    for doc in samples:\n",
    "        tokens.append(model[doc])\n",
    "        labels.append(movies_dict[doc])\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=50, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    #plt.xlim(-4.0, -1.0)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sample_tags = ['harry_potter_and_the_chamber_of_secrets_(film)',\n",
    "'harry_potter_and_the_prisoner_of_azkaban_(film)',\n",
    "'harry_potter_and_the_goblet_of_fire_(film)', \n",
    "'harry_potter_and_the_half-blood_prince_(film)',\n",
    "'harry_potter_and_the_order_of_the_phoenix_(film)']\n",
    "doc_tags = list(model.docvecs.doctags.keys())\n",
    "bad_sample = doc_tags[100:130]\n",
    "samples = set(sample_tags + bad_sample)\n",
    "tsne_plot_doc2vec(model, samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = model['harry_potter_and_the_chamber_of_secrets_(film)'] - model['harry_potter_and_the_prisoner_of_azkaban_(film)']\n",
    "print(x)\n",
    "sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = model['harry_potter_and_the_chamber_of_secrets_(film)'] - model['just_go_with_it']\n",
    "sum(x)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsne = TSNE(n_components=2)\n",
    "X_tsne = tsne.fit_transform(sample_docs)\n",
    "df = pd.DataFrame(X_tsne, index= sample_tags, columns=['x', 'y'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#the_bride_wore_red\n",
    "m1 = helper_fn.find_movie_sent_by_id(\"the_beautiful_person\", train_data_df)\n",
    "m2 = helper_fn.find_movie_sent_by_id('second_fiddle_(1957_film)', train_data_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(m1)\n",
    "print(m2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsne_d2v\n",
    "\n",
    "plt.scatter(X_tsne[:, 0], X_tsne[:, 1])\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def tsne_plot_word2vec(model):\n",
    "    \"Creates and TSNE model and plots it\"\n",
    "    labels = []\n",
    "    tokens = []\n",
    "    \n",
    "\n",
    "    for word in model.wv.vocab[0:5000]:\n",
    "        tokens.append(model[word])\n",
    "        labels.append(word)\n",
    "    \n",
    "    tsne_model = TSNE(perplexity=40, n_components=2, init='pca', n_iter=2500, random_state=23)\n",
    "    new_values = tsne_model.fit_transform(tokens)\n",
    "\n",
    "    x = []\n",
    "    y = []\n",
    "    for value in new_values:\n",
    "        x.append(value[0])\n",
    "        y.append(value[1])\n",
    "        \n",
    "    plt.figure(figsize=(16, 16)) \n",
    "    for i in range(len(x)):\n",
    "        plt.scatter(x[i],y[i])\n",
    "        plt.annotate(labels[i],\n",
    "                     xy=(x[i], y[i]),\n",
    "                     xytext=(5, 2),\n",
    "                     textcoords='offset points',\n",
    "                     ha='right',\n",
    "                     va='bottom')\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_df.isnull().any()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movies_df.describe"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_in_movie_sent(search_list, row):\n",
    " #   print(row)\n",
    "    if all(elem in row['movie_sentence']  for elem in search_list):\n",
    "        return(row['movie_id'])\n",
    "    else:\n",
    "        return False\n",
    "        \n",
    "search_list=[ 'adventure', 'family','chris_columbus', 'j.k._rowling']\n",
    "x=movies_df.apply(lambda row: find_in_movie_sent(search_list, row),axis=1)\n",
    "#doc_tags =[]\n",
    "for item in x:\n",
    "    if item != False:\n",
    "        doc_tags.append(item)\n",
    "        \n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "doc_tags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tsne_plot(list(set(tags)),model)\n",
    "#docs_vectors.doctags"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#model['going_overboard_tt0096870']\n",
    "sim = docs_vectors.most_similar([model['billy_madison_tt0112508']])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "tags=[] \n",
    "for id, similarity in sim:\n",
    "    tags.append(id)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "print(*doc_tags, sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "movie_sent_dict = dict(zip(movies_df.movie_id, movies_df.movie_sentence))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "target_id = 'the_hunger_games_tt1392170'\n",
    "n_items= 5\n",
    "treshold=.4\n",
    "data_size= 10\n",
    "\n",
    "topn_list = model.docvecs.most_similar([target_movie_id], topn= n_items + 1)\n",
    "#remove the target movie from the list\n",
    "for idx, sim in enumerate(topn_list):    \n",
    "    if sim[0] == target_id:        \n",
    "        topn_list.pop(idx)\n",
    "#(topn_list.pop(idx) if (topn_list[0] == target_movie_id ) for idx, sim in enumerate(topn_list))\n",
    "precision_df = calculate_precision(target_id, topn_list, movie_sent_dict, treshold)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.docvecs.most_similar([model[target_id]], topn=5+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "model.docvecs.most_similar([target_id], topn=5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "random.seed = 400\n",
    "n_items= 5\n",
    "treshold=.44\n",
    "data_size= 100\n",
    "random.seed = 150\n",
    "dict_keys = list(movie_sent_dict.keys())\n",
    "random.shuffle(dict_keys)\n",
    "movie_id_list = random.sample(dict_keys, data_size)\n",
    "#movie_id_list=[\"the_hunger_games_tt1392170\", \"harry_potter_and_the_sorcerer's_stone_tt0241527\"]\n",
    "precision_df = calculate_precision_data_sample(5 , 4 ,movie_id_list, movie_sent_dict, treshold )\n",
    "#movie_id_list\n",
    "\n",
    "precision_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_vectors.most_similar(['harry_potter_and_the_sorcerer\\'s_stone_tt0241527'])\n",
    "docs_vectors.most_similar(['survivor_tt3247714'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "movies_df.sample(frac=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_movies_df = train_data_df.loc[train_data_df.apply(lambda movie: True if ('romantic_comedy' in movie['genre']) \n",
    "                                                 and ('adam_sandler' in movie.actor_name) else False, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_movies_df[['title','genre','actor_name','director_name','writer_name']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#drama_movies_df = movies_df.loc[movies_df.genre.apply(lambda genre: True if 'drama' in genre else False)]\n",
    "\n",
    "common_attrs_df= pd.DataFrame(columns=['movie_id','actors_cnt', 'directors_cnt','writers_cnt','genres_cnt','total_cnt'])\n",
    "\n",
    "sample_movies_df.reset_index(drop=True, inplace=True)\n",
    "print(query_movie_id)\n",
    "query_movie_id= sample_movies_df.iloc[2].movie_id\n",
    "\n",
    "common_attrs_df= common_attrs_df.append(sample_movies_df.progress_apply(lambda row: find_common_attrs(sample_movies_df,query_movie_id, row.movie_id), axis=1)\n",
    "                                        ,ignore_index = True)\n",
    "\n",
    "print(common_attrs_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_movies_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_movies_df = train_data_df.loc[train_data_df.apply(lambda movie: True if ('romantic_comedy' in movie['genre']) \n",
    "                                                      and ('adam_sandler' in movie.actor_name) else False, axis=1)]\n",
    "\n",
    "cols=['actor_name','director_name', 'writer_name','genre','country']\n",
    "common_attr_matrix = sample_movies_df.apply(lambda row: eval_fn.find_common_attrs_in_list(sample_movies_df, row.movie_id, cols), axis=1)\n",
    "common_attr_matrix.set_index('title', inplace= True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_attr_matrix['Title'] = common_attr_matrix.Title.str.capitalize()\n",
    "common_attr_matrix.columns = common_attr_matrix.columns.str.capitalize()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "common_attr_matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "sample_movies_df = movies_df.loc[movies_df.apply(lambda movie: True if ('romance' in movie['genre']) \n",
    "                                                 and ('comedy' in movie.genre) and ('adam_sandler' in movie.actor_name) else False, axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_attrs_df = common_attrs_df[common_attrs_df.movie_id != query_movie_id]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "common_attrs_df[common_attrs_df.total_cnt == common_attrs_df.total_cnt.max()]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "docs_vectors.most_similar('the_wedding_singer_tt0120888', topn=30)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helper_fn.find_movie_sent_by_id('a_night_at_the_roxbury_tt0120770',movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helper_fn.find_movie_sent_by_id('the_wedding_singer_tt0120888',movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "helper_fn.find_movie_sent_by_id('mr._deeds_tt0280590',movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_common_attrs(movies_df,'the_wedding_singer_tt0120888', 'mr._deeds_tt0280590')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "find_common_attrs(movies_df,'the_wedding_singer_tt0120888', 'a_night_at_the_roxbury_tt0120770')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "def find_common_attrs_dict(movies_df,query_movie_id, search_movie_id):\n",
    "    query_movie= movies_df[movies_df.movie_id == query_movie_id]\n",
    "    search_movie= movies_df[movies_df.movie_id == search_movie_id]\n",
    "    \n",
    "    common_actors= count_sim(query_movie, search_movie, 'actor_name')\n",
    "    common_directors= count_sim(query_movie, search_movie, 'director_name')\n",
    "    common_writers= count_sim(query_movie, search_movie, 'writer_name')\n",
    "    common_genres= count_sim(query_movie, search_movie, 'genre')\n",
    "    \n",
    "    common_dict = {search_movie_id: common_actors+common_directors+common_writers+common_genres}\n",
    "    return common_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "    common_attrs_dict=[]\n",
    "    common_attrs_dict = [movies_df.progress_apply(lambda row: find_common_attr(movies_df,query_movie_id, row.movie_id), axis=1)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "drama_movies_df = movies_df.loc[movies_df.genre.apply(lambda genre: True if 'drama' in genre else False)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "len(drama_movies_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "x = train_data_df.apply(lambda row: len(row.actor_name) + len(row.director_name)+len(row.writer_name)+len(row.genre)+len(row.country), axis=1)\n",
    "y = train_data_df.apply(lambda row: len(row.movie_sentence) ,axis=1)\n",
    "df=pd.DataFrame(data=[x,y])\n",
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "data_slice= search_list[start:end] \n",
    "data_chunck =[]\n",
    "print(\"Searching by movie title\")\n",
    "data_chunck = [movie.strip().lower() for movie in data_slice]\n",
    "final_search_list = '\\\"\"\"' + '\\\"\"\",\\\"\"\"'.join(data_chunck) + '\\\"\"\"' \n",
    "try:\n",
    "    results = execute_query_for_movies_list(query,final_search_list,endpoint, return_format,request_method)              \n",
    "    all_results_df = all_results_df.append(get_sparql_dataframe(results))  \n",
    "except Exception as ex:\n",
    "    print(ex)\n",
    "    print(\"Chunck Failed : {}\".format(final_search_list))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "m='just_go_with_it'\n",
    "for doc in tagged_documents:\n",
    "    if m in doc.tags:\n",
    "        d = doc\n",
    "        \n",
    "d"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
